[{"authors":["admin"],"categories":null,"content":"このブログはあくまで会社もしくはその社員としての公式見解の場ではありません。全ての記事は個人としての発言です。このブログでは、リアルタイムグラフィックス関連で、興味のあることを取り扱っていきたいと思います。\n以前の記事はこちらにあります。 https://shikihuiku.wordpress.com/\nThis BLOG is a place to express things which I’m interested in, especially related to computer graphics, not for a place to discuss about my work or company. All things noted on this BLOG are based on my personal views.\nThank you.\nOld site is here, https://shikihuiku.wordpress.com/\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shikihuiku.github.io/author/shikihuiku/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shikihuiku/","section":"authors","summary":"このブログはあくまで会社もしくはその社員としての公式見解の場","tags":null,"title":"shikihuiku","type":"authors"},{"authors":[],"categories":[],"content":"前提知識として、 RTXDIのminimal-sampleをSpatioTemporal Resamplingなしの場合の動作について理解する必要があります。\nRISとReSTIR minimal-sampleは、まず初めに、現在レンダリングしているフレーム内でLight SampleとBRDF SampleをMISで結合したRservoirを生成します。この時点でもReservoirの結合を行いますが、基本的には Resampled Importance Sampling: RISのアルゴリズムに基づいて最適なライトパスの選択が行われます。\n加えて、\u0026ldquo;Enable Resampling\u0026quot;チェックボックスを有効にした場合は、現在のフレームで生成したReservoirと、前のフレームで生成されたReservoirを結合することで、さらに良質なライトパスの選択を行うことが出来ます。この処理を、Reservoir-based SpatioTemporal Importance Resampling: ReSTIRと呼びます。\nReSTIRの効果 端的にReSTIRの効果の有無を比較すると以下のようになります。ReSTIRの処理が追加されるので処理負荷は大きくなりますが、もしもReSTIRを使わずに、これと同等のレンダリングを達成するためには、ずっと多くの処理時間が必要となるでしょう。   RIS+ReSTIR     RIS   RTXDI_SpatioTemporalResampling()の処理 RTXDI SDKのReSTIRの処理は、RTXDI_SpatioTemporalResampling()関数で行われます。minimal-sampleではRender.hlslから呼ばれています。引数には、以下の情報を渡します。返り値として、ReSTIRで結合されたReservoirが返されます。\nreservoir = RTXDI_SpatioTemporalResampling(pixelPosition, primary.surface, reservoir, rng, stparams, params, temporalSamplePixelPos, lightSample);\n pixelPosition  現在処理をしているPixelの位置   surface  現在処理をしているPixelのサーフェース情報（位置, 法線 tec..）   curSample  現在のフレームで生成したReservoir   rng  乱数生成用のステート   stparams  Spatio Temporal Resamplingの処理に関するのパラメーター（後述)   params  RTXDI SDKの定数パラメーター（バッファのオフセット情報など）   temporalSamplePixelPos  Backprojectionに成功した場合は、そのピクセル位置が格納されます。失敗すれば(-1,-1)が格納されます。   selectedLightSample  Spatio Temporal Resamplingの処理でReservoirの選択サンプルが更新された場合は、このライトサンプルの情報が更新されます。    RTXDI_SpatioTemporalResamplingParameters 構造体 RTXDI_SpatioTemporalResampling()関数を呼び出す際の引数にあるこの構造体には、 ReSTIRの制御に関する様々なパラメーターが格納されています。\n screenSpaceMotion  現在処理しているピクセルのモーションベクトルです   sourceBufferIndex  Reservoirバッファのフレームごとの参照オフセットを計算するためのインデックスです。   maxHistoryLength  結合されたReservoirのウエイトの上限を決めます。この値が大きいほど、過去に多数のReservoirと結合されたサンプルのウエイトが高くなります。 また、逆を言えば、シーンの変化への追従が悪くなります。   biasCorrectionMode  Reservoir結合時のBiasの補正方法です。 RTXDI_BIAS_CORRECTION_OFF  Biasの補正をしない結合方法を使います。処理は一番軽いですが、レンダリング結果にBiasを導入します。   RTXDI_BIAS_CORRECTION_BASIC  TargetPDFを再計算してBiasを補正しますが基本的に結合されたReservoirはすべて有効であると仮定します。異なる場合はBiasが導入されます。   RTXDI_BIAS_CORRECTION_PAIRWISE  pairwise MISという方法でBiasを補正します。基本的に結合されたReservoirはすべて有効であると仮定します。異なる場合はBiasが導入されます。今回の記事では説明しません。   RTXDI_BIAS_CORRECTION_RAY_TRACED  TargetPDFを再計算してBiasを補正したうえで、レイトレースを行い結合されたReservoirが有効かどうかをチェックします。基本的にBiasを導入しない方法です。     depthThreshold, normalThreshold  Backprojectionをしたときに、法線とデプスの相似度をチェックする際の閾値です。   numSamples  結合を試みるReservoirの数です。最低1必要で、最初の一つは、TemporalResamplingとなります。   numDisocclusionBoostSamples  Backprojectionに失敗した場合に、SpartialSampleの数を増やす場合のサンプル数です   samplingRadius  SpartialSampleのサンプリング半径（ピクセル単位）です。   enableVisibilityShortcut  RTXDI_BIAS_CORRECTION_RAY_TRACEDの時のみ有効です。 Reservoir結合後に、Visibilityテストを行う際にTemporalSampleが選択された場合はVisibilityテストをスキップします。  （ここのIfの判定は不明。おそらくだが、選択されたサンプルのReservoirのVisibilityテストをスキップするのが正しいと思う。（なぜならそれは前フレームで行ったから。））     enablePermutationSampling  BackprojectionとSpartialSamplingの位置にに小さいオフセットを適用します。    Backprojectionの処理 まず、前のフレームのReservoirと結合するためには、Backprojectionの処理を行う必要があります。この処理自体は、通常我々が行っているものと違いはありません。モーションベクトルを基に、過去フレームのサンプル位置を算出し、その近傍で、法線やDepthの類似性が高いサンプルを探します。\nTemporal Sampleの読み出し Backprojectionが成功したら、Reservoirバッファより、前フレームのPixel位置に対応するReservoirを読み出してprevSampleに格納します。読み出されるのは前のフレームに保存されたReservoirの情報になります。読み出したReservoirに対して以下の処理を行います。\n MをhistoryLimitでクランプ spartialDistanceにピクセルオフセットを加算 ageをインクリメント lightIDを現在のフレームのライトバッファに対応するIDに変換  lightIDの変換では、もし該当するライトが、現在のフレームになければ読み出したReservoirを破棄します。    Temporal Sampleの結合 prevSampleが有効なReservoirだった場合は、現在のフレームで生成されたReservoirと結合します。 まず、prevSampleのReservoirの情報を基に、Light Sampleを構築します。ここで構築されるLight Sampleは、読み出し時に、ligtIDを変換したので、現在のフレームにおける光源サンプルの位置になります。そして、現在処理をしているSurfaceとそのLight Sampleで、targetPDFを計算します。（つまりシェーディングの計算をします。）この値は、前のフレームのReservoirを結合する際の、ウエイトの補正に使います。\n結合の計算の詳細:\nまず、prevSampleはFinalizeされて格納されているので、そのメンバー変数weightSumは意味的には、(1/targetPDF * 1/M * weightSum)の値になっています。 そして、prevSampleのRIS Weightは、プログラム上では(RTXDI_CombineReservoirs()呼び出しの引数のtargetPdf) * (Reservoirのメンバー変数のweightSum) * (Reservoirのメンバー変数のM)で計算されます。\nこれを意味的に解釈すると(引数のtargetPDF)/(元のtargetPDF) * weightSum となります。 つまり本来の意味でのweightSumに、新旧のtargetPDFの比を乗算したものがRISWeightとして使われることになります。 結合後は、MとRISWeightはそれぞれ結合先のReservoirに加算され、乱数によるサンプルの選択が行われることで結合が完了します。\nSpartial Sampleの読み出しと結合 minimal-sampleにおいて、Spartial SampleはTemporal Sampleと同様に、前フレームのGBufferとReservoirバッファから読み出されます。 テストするサンプル数は、デバッグUI上のSpartial Sampleのスライダーで調整できます。Temporal Sampleとの主な違いは、Backprojectionした位置から、さらに、NeighborOffsetBufferから取得した値でオフセットを適用するところにあります。NeighborOffsetBufferはSDK側からその内容があらかじめ提供されている静的なバッファで、Spartial Sampleのサンプリングパターンが格納されています。読み出したサンプルは、Normal, Dpethそして、GbuffのMaterialの相似度をみて、サンプルが有効かを判定します。\n有効な場合は、Temporal Sampleの場合と同様にReservoirを結合行います。結合の計算は、Temporal Sampleの場合と同じです。\nBiasの補正とReservoirのFinalize処理 隣接ピクセルや、過去のフレームのReservoirとの結合はBiasを発生させることがあります。例えば、異なるピクセルで生成した複数のReservoirを結合した場合、個々のピクセルの積分範囲（法線を中心とした半球）は異なるため、もし、結合後に選択したLight Sampleが、結合された、とあるReservoirの積分範囲の外であったり、不可視な状態だったなら、このReservoirからMの値を、Fianlize処理するときの分母に含めてはいけません。そうしないと、Biasが発生してしまいます。（詳しくはReSTIRの論文を参照）\n可視状態の確認は、biasCorrectionModeにRTXDI_BIAS_CORRECTION_RAY_TRACEDが設定された場合に実行されます。具体的には、選択されたLight Sampleの位置と、各Reservoirの位置を、過去のフレームのBVHでレイトレースして、可視状態を確認します。（ただし、サンプル内の実際の処理では、シーンがスタティックであると仮定して、単純に現在のフレームのBVHでレイトレースするように実装されています。）\n次にFinazlieの処理についてです。RTXDI_SpatioTemporalResampling()関数の正規化部分では、piとpiSumという変数が、Finalizeする際の係数の分子と分母になるように記述されています。もしも、ここがもっと単純な記述だったら、piは常に1で、piSumには、選択したLight SampleがそのReservoirの積分範囲内で、かつ可視状態の、有効なReservoirのMのみを加算することで、Finalize処理の係数を算出する形になります。（ReSTIR論文における1/Zに相当）\nしかし、RTXDIでは、MISのように正規化の係数を計算しています。具体的には、選択されたLight Sampleと各Reservoirの位置で、targetPDFを計算し、可視状態ならば(targetPDF*M)という値を分母側のpiSumに蓄積しています。分子側のpiは選択されたLight Sampleを保持していたReservoirとのtargetPDFです。つまり、選択されたLight Sampleを保持していたReservoirのtargetPDF(つまりはシェーディングの輝度）が相対的に他のReservoirと計算した輝度よりも高ければ、Finalizeする際の係数が大きくなるように計算されています。\nまた、piとpiSumの初期値は、Temporal SampleやSpatial Sampleとの結合前の、現在のフレームで計算されたReservoirの値を設定します(curSample.M)。これのMISのウエイトとしてstate.targetPdfを使っています。これは、現在選択されているLight Sampleと、現在処理中のサーフェースで計算されたtargetPDFで、この値は、他のTemporal SampleやSpatial Sampleのために計算するtargetPdfに対応する値です。（この値は、現在のフレームのデータで計算しています。他のTemporal SampleやSpatial SampleのReservoirのtargetPdfは前フレームのデータで計算しているという点は異なります。）\nループ処理が完了すれば、結合されたすべてのReservoirのBiasの除外のチェックが完了したことになります。そして、piには選択されたサンプルのtargetPDFが格納され、piSumにはtargetPDF*Mの総和が格納されています。 pi/piSumを正規化係数としてFinalize処理を行うことで、Biasの補正をした結合ができます。\nBiasの補正をしない場合のReservoirのFinalize処理 Biasの補正をしない場合は、単純に結合されたReservoirを1/Mを正規化係数として、Finalize処理します。\nまとめ 最後まで読んじゃった人は「にゃ～ん」ってつぶやいてほしいです。\n","date":1655704480,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655704480,"objectID":"97165321a5774fb54f155b7f3b998b36","permalink":"https://shikihuiku.github.io/post/rtxdi_second_step/","publishdate":"2022-06-20T14:54:40+09:00","relpermalink":"/post/rtxdi_second_step/","section":"post","summary":"最も簡単なサンプルを通じてRTXDIとReservoir-based SpatioTemporal Importance Resampling (ReSTIR)の基礎を理解する","tags":["RayTracing","Direct3D"],"title":"RTXDIのminimal-sampleを理解する(2)","type":"post"},{"authors":[],"categories":[],"content":"RTXDIとは？ GPU上かどうかにかかわらずレイトレーシングやパストレーシングを行う際の重要な課題の一つは、追跡する光線の軌跡（パスもしくはレイと呼ばれるもの）をどのように構築するかです。これはレンダラーの性能や画質などの特性に直結する問題です。たとえば、物体表面からの反射に限定すれば、最も簡単なパスの構築方法は、物体の表面から半球状ににランダムな方向を選択してパスを構築する方法があると思います。また、物体表面の反射特性に合わせて、より反射率の高い方向を高確率で選択する方法や、シーン上に存在する光源の方向にパスを構築する方法もあります。\nこのように、いろいろなパスの選択戦略があり、実際のレンダリングでは、これらを組み合わせて使うことがよくある思います。そして、最も理想的なパスの確率分布は、その物体表面から、観測者の方向へのRadianceに比例した確率分布といわれています。しかしこれは、一般的には解析的に解くことが極めて困難であることがほとんどです。なぜなら、物体表面の反射特性は分かっても、どの方向から強い光が差し込んでくるかはわかりません。その光も、シーン上に設定された光源からの直接光なのか、それとも何かほかの物体から反射された光なのかわかりません。\nRTXDIは、光源からの直接光によって形成されるRadianceに対して、最適なパスの確率分布を形成するようにパスの選択をするためのNVIDIAのSDKです。名前の由来は、おそらくRTX Direct Illuminationです。\nリポジトリ GitHubのリポジトリがあるので、さっそくCloneしてみましょう。\n以降の説明では基本的にCloneしたソースを読める状態にある前提で書いています。\n https://github.com/NVIDIAGameWorks/RTXDI/\nドキュメント RTXDIのSDKのドキュメントを見る前に、前提知識として、Resampled Importance Sampling(RIS)のアルゴリズムの基礎部分を理解した方がよいと思います。（これより先は、Resampled Importance SamplingをRISと省略します。）\n\u0026ldquo;Importance Resampling for Global Illumination\u0026rdquo; by J. Talbot et al.\nRTXDIのSDKには、その概要を把握するのに下記のドキュメントがありますが、これを読んで理解できる人は、この記事はここで読むのを終了していただいて、SDKのドキュメントやソースコードを直接参照した方が良いでしょう。 https://github.com/NVIDIAGameWorks/RTXDI/blob/main/doc/Integration.md\nrtxdi-sampleとminimal-sample このSDKにはサンプルプロジェクトが二つ付いています。\nrtxdi-sampleは、RTXDIをパス選択の核として、RTXGIやNRDやDLSSを用いてレンダリングしています。またReGIRという、ワールド空間におけるRISも行っているので、かなり実践的なサンプルになっている一方で、初めのステップとして、RTXDIの動作を理解したい場合には不向きなサンプルです。\n一方で、minimal-sampleは、設定を変更することで時間方向のRISや、BRDFに基づくサンプリングも無効にすることが出来ます。また、NRDによるデノイズも行っておらず、レンダリングは極力単純な形で留めてあります。そのため、RTXDIの核であるRISの仕組みや、その効果をわかりやすく見せてくれるサンプルになっています。本記事ではこちらのサンプルプログラムの動作を見ていきます。\nminimal-sampleのスクリーンショット 端的にRTXDIの効果の一端を見るためにいくつかのスクリーンショットを用意しました。   1サンプル     8サンプル     16サンプル   まずは上記3枚は、RTXDIのパス選択候補を、8サンプル、16サンプルと増加させたものです。選択候補は増やしているのですが、実際にれらのサンプルでレイトレースを行った訳ではありません。レイトレースはあくまで1回のみ行います。\n   16サンプル＋BRDF2サンプル   次は、16サンプル+BRDF2サンプルの場合です。こちらはレイトレース回数は、合計3回となります。BRDFサンプルによって、良い選択候補が見つかるサーフェースでの変化が顕著に見られます。\n   4サンプル＋BRDF1サンプル+Spatio-Temporal Resample   今回の記事では説明しませんが、Spatio-Temporalのパス選択候補を導入すると、上記のようになります。上記の16サンプルと同等の処理時間ですが、結果は圧倒的にこちらが優れています。デノイズ処理は一切入っていない状態でここまでレンダリングできれば、かなり高画質なレンダリングが期待できます。\nminimal-sampleを読む前に（前提知識） ここでサンプルプログラムのレンダリングを見る前に、簡単に触れておいた方が良い前提知識について説明します。\nNVRHIとDonutフレームワーク RTXDI SDKのほかに、minimal-sampleが依存している主なライブラリとして、DonutとNVRHIがあります。\nNVRHIは、D3D12とVulkanを抽象化するためのグラフィックスAPIの抽象化レイヤーです。とはいえそれほど深い抽象化が行われているわけではありません。\nDonutは、サンプルアプリケーションのフレームワークに相当する部分になります。シーンのロードやシェーダーの管理、デバッグUIの表示などを行っています。こちらもサンプル向けのフレームワークなので、シンプルに記述されています。今回のサンプルプログラムでは、それほど多数のDispatchが呼び出されるわけではないので、動作の理解に苦しむことはないかと思います。\nRAB_プレフィックスについて RTXDIのサンプルを見ると、RTXDI_プレフィックスの関数や構造体とは別に、RAB_プレフィックスの関数や構造体がたくさんあります。RABの意味はRTXDI Application Bridgeという意味で、その名の通り、RTXDIとアプリケーションの橋渡しの役目があります。\nRTXDIがRISを行うときに必要になる情報は、アプリケーションのレンダラーと密接に関係しています。そのため、RTXDIがアプリケーション由来の情報と思われるものを取得する際は、RAB_プレフィックスのついた関数を呼び出します。RTXDIが呼び出している、RABプレフィックスのついた関数を実装するのは、アプリケーション側の責任となります。 しかし実際は、サンプルアプリケーションのRAB実装である、RtxdiApplicationBridge.hlslを改変する形で自身のアプリケーションに組み込む形になると思います。このようにすることで、アプリケーションごとに改変の必要な部分と不要な部分の切り分けを実現しています。\nRTXDI特有のリソース 通常のG-Bufferなどに加えて、minimal-sampleでRTXDI SDKを導入したことで必要となるリソースは以下の通りです。RTXDIは、SDKの内部でリソースを確保することはありません。リソースの管理は、生成、破棄を含め、すべてアプリケーション側の管理となります。 SDK側からは必要に応じて、リソースのサイズやその中身がAPIを通じて提供されるので、アプリケーションはそれらを正しく管理しなくてはなりません。 以下のリソースは、ソースコード全体の把握では大切な要素ですが、RISのアルゴリズム部分ではあまり関わりが無いので読み飛ばしても問題ありません。\n  TaskBuffer\nRTXDIは、毎フレーム直接光源情報のテーブルの更新を行っています。これはPrepareLightというGPU処理マーカーの中でComputeShaderとして行われています。この処理の入力として TaskBufferが必要となります。このバッファはPrepareLightsTask構造体の配列となっています。 このサンプルでは、シーン上でEmissiveサーフェースを持ったGeometry Instanceの個数分のバッファを確保しています。\n  LightBuffer\nRTXDIがアクセスする光源の情報はすべてこのバッファに格納されます。個々の光源は、RAB_LightInfo構造体に格納されます。 このサンプルでは、Emmisiveのマテリアルが設定されたポリゴン一つ一つがEmissiveTriangleの光源としてこの配列に設定されます。 TaskBufferによって入力された情報をもとに、このバッファが構築されます。\n  GeometryInstanceToLightBuffer\nGeometry Instanceごとに、そのInstanceに含まれるEmissiveTriangleの光源としてのLightBufferにおける先頭のインデックスを格納します。つまり、GeometryInstanceのインデックスからLightBufferを参照するときに使われるテーブルです。\n  NeighborOffsetBuffer\nRTXDIがサイズを提供しと内容を指定します。スクリーンスペースでRISを行うときに参照するべきPixelへのオフセットになる値が格納されます。EvenとOddのフィールドがあるのでRTXDIが提供するNeighborOffsetCountの2倍の数で、RG8_SNORMのTypedBufferを確保します。 レンダリングの前に、RTXDIのFillNeighborOffsetBuffer()で取得できるバイト列をこのバッファに書き込む必要があります。\n  LightReservoirBuffer\nRTXDIがサイズを提供し内容はComputeShaderで算出されます。 ReservoirBuffer一つあたりのサイズはsizeof(RTXDI_PackedReservoir) * context.GetReservoirBufferElementCount()で、RTXDIから提供されます。 これをアプリケーション側の好きな数だけ確保します。サンプルの初期値では3セット分のサイズのバッファを確保しています。 時間方向でRISを行う場合のためのバッファになります。\n  RTXDIのReservoirについて （ここより以下、RTXDIもしくはRISの文脈で、\u0026ldquo;サンプル\u0026quot;と言っている場合は、レイトレーシングにおけるサーフェースと光源を結ぶパスを構築するためのLight Sampleを指します。サンプルアプリケーションのことでもなければ、テクスチャのサンプリングのことでもありません。）\nRTXDI_Reservoir構造体はRISのReservoirとしての情報を保持します。Reservoirとは、RISをするためのサンプルの集合です。ただし、サンプルの集合の情報をすべて保持していたら、GPU上ではメモリが足りません。したがって、Reservoirは今まで生成してきたサンプルによる確率の計算と、現在そのReservoirで選択されているサンプルの情報を格納しています。具体的には、サンプルの選択確率に関する情報と、パスの接続対象なる光源のインデックス、その光源の表面における位置情報にあたるUVです。これがあれば、ワールド空間でパスを接続するべき位置（つまりは光源の表面位置）が計算でき、シェーディングを行った後にサンプルの確率密度を適用することができます。\nReservoirに関して全くイメージがわかないという場合は、まず初めに紹介した論文を軽く読んで、ReSTIRに関する論文、 \u0026ldquo;Spatiotemporal reservoir resampling for real-time ray tracing with dynamic direct lighting\u0026rdquo;, Bitterli et al. 2020\nを読むとイメージできると思います。（もしこの二つを読んだならば、本記事は、この先読む必要がないでしょう）\nReservoirを操作する関数群 ここではRTXDIがReservoirを操作する関数群のなかで、最も基本的なものをザックリと説明します。\n  RTXDI_Reservoir RTXDI_Reservoir RTXDI_EmptyReservoir()\n有効なサンプルが一つも格納されていない、初期化されたRTXDI_Reservoir構造体を返します。\n  bool RTXDI_StreamSample( inout RTXDI_Reservoir reservoir, uint lightIndex, float2 uv, float random, float targetPdf, float invSourcePdf)\n reservoir - 格納するReservoir lightIndex, uv - 追加するLight Sampleの情報 random - Light Sampleを更新するかどうかをDraw（選択）するときに使う乱数 targetPdf - RISにおけるTarget PDF invSourcePdf - 追加するサンプルを生成する確率の逆数  一つのサンプルをReservoirに追加して、現在このReservoirの中で選択されているサンプルを更新します。\ntargetPDFは実際は正規化されたPDFである必要はなく、単なるウエイト値で問題ありません。一方で、invSourcePdfは、サンプルの発生確率に基づいたPDFである必要があります。関数内部では、RIS Weightが targetPdf * invSourcePdf で計算され、Reservoir構造体の weightSum に加算されます。Reservoirの保持サンプル数Mもインクリメントされます。また、与えられた randomでサンプルの選択を行い、新たに追加されたサンプルが選択された場合はReservoir内部の選択サンプルの情報を更新します。その場合は、返り値としてtrueを返します。\n  void RTXDI_FinalizeResampling( inout RTXDI_Reservoir reservoir, float normalizationNumerator, float normalizationDenominator) \n通常は、Reservoirへのサンプル追加が終わった段階で呼び出す処理で、Reservoirに蓄積されたサンプルの確率と、現在選択されているサンプルの確率から、選択されているサンプルの評価値（つまりはシェーディング結果）に乗算するべき値 (Importance Samplingにおける 1/PDF) を計算します。\nnormalizationNumerator, normalizationDenominatorは蓄積されたサンプルのweightSumを正規化するときの係数です。単独のReservoirであれば、Reservoirに蓄積されたサンプル数の逆数である、1/Mが係数として適切です。この場合、1/targetPDF * (1/M * weightSum)を計算し、これを weightSum に代入します。\nしたがって、この関数を呼び出す前と後では構造体メンバーのweightSumの値の意味が変わります。呼び出す前はReservoirに蓄積されたサンプルのウエイトの合算で、呼び出した後は、選択されたサンプルの評価値に乗算するべき値となります。\n  float RTXDI_GetReservoirInvPdf(const RTXDI_Reservoir reservoir)\nSampleの評価値に乗算するべき係数（Importance Samplingにおける1/PDF）を返します。\n内部の処理はweightSumの値を返すだけです。事前にFinalizeResampling()を呼ぶ必要があります。\n  bool RTXDI_CombineReservoirs( inout RTXDI_Reservoir reservoir, const RTXDI_Reservoir newReservoir, float random, float targetPdf)\n二つのReservoirを結合します。\nまず、結合前に結合される側のnewReservoirはRTXDI_FinalizeResampling()で正規化されている必要があります。\n引数targetPdfは、結合されるnewReservoirで選択されているサンプルの、結合先ReservoirにおけるtargetPdfになります。結合される側と結合先でのtargetPdfが同じ場合は、引数のtargetPdfは、newReservoirに保存されているサンプルのtargetPdfを指定すればよいです。\n結合後は、現在どちらかのReservoirで選択されているサンプルが選択サンプルになります。これをrandomを用いて決めます。選択サンプルが変更される場合は返り値としてtrueを返します。\n  minimal-sampleの中身\n~Spatio-TemporalでRISを行わない場合のレンダリング~ レンダリングを理解するうえでの前提知識が整ったので、さっそく一番簡単なケースのレンダリングを見たいと思います。 Spatio-TemporalでのRISは、RTXDIの大きな特長の一つですが、今回は単純化のために無効化した状態でサンプルコードを読み、 RTXDIの最もシンプルな形を理解するこにします。このサンプルアプリケーションは、\u0026ldquo;Enable Resampling\u0026quot;というDebugUIが用意されているのでこれをDisableにします。しかしこれはSpatio-TemporalのRISを行うかどうかを切り替えるためのフラグで、RTXDIを完全にDisableにするためのものではありません。また、BRDF Cutoffも、簡単のため0.0が設定されていると仮定します。\n  Settings   レイトレーサー本体の概要 Renderer.hlslのmain()がレイトレーサー本体のシェーダーコードです。カメラからレイを飛ばして、GBuffer相当の情報を取得している部分は特に難しい部分はないと思います。サーフェースにヒットした場合は、乱数シーケンスを初期化して、RTXDI_SampleParamterにサンプリングの設定をしています。その後の主な処理の流れは以下の通りです。\n 空のReservoirに、RTXDI_SampleLocalLights()(後述)で計算されたReservoirを結合する RTXDI_SampleBrdf()(後述)で計算されたReservoirを結合する RTXDI_FinalizeResampling()でReservoirの正規化を行う 選択パスが、RTXDI_SampleLocalLights()だったら、ShadowRayをキャストして、Visibilityをチェック ShadeSurfaceWithLightSample()で、Reservoirで選択されたサンプルを使ってシェーディングを行う 再びShadowRayをキャストしてVisibilityをチェック  Spatio-TemporalのRISが無効化されている場合は、最後の2度目のVisibilityチェックは必要ないはずです。しかし、大まかな処理の流れとしてはこのようになっています。以上を簡単に言い換えれば、1バウンスのライトサンプル(NEE)と、BRDFサンプルのMulti Importance Samplingのレイトレーサーが実装されているといえると思います。\nRTXDI_SampleLocalLights() さっそくですが、1番めの処理についてです。この関数はResamplingFunctions.hlsliに実装されています。 この関数は、numLocalLightSamplesで指定された数だけ、サンプルを構築してReservoirに蓄積する処理を行います。このサンプルアプリケーションの中の様々な個所で行われているRISの最も基本的な形になっています。\n個々のサンプルの構築 RTXSDKは事前にLight DataバッファにLocal Light (つまりは Emissive Triangle)のリストを構築しています。まず、このリストから、単純に乱数でLocal Lightを選択します。さらに乱数を2つ生成して、光源の三角形上の点を決定して、その位置に向けて、プライマリレイがヒットしたサーフェースからパスを構築します。\n構築されたパスのPDFは、RTXDI_LightBrdfMisWeight()で計算され、blendedSroucePdfに代入されます。\nblendedSourcePdfの計算 blendedSourcePdfは、RISにおけるsourcePDFなので、実際のパス生成確率に即したものでなければなりません。 この計算を行っているのは、RTXDI_LightBrdfMisWeight()関数です。\nまず、ライトサンプルの確率は\n ライトの選択確率（単なる乱数選択なので、ライトの個数の逆数） ライト上の特定の方向に向けたレイを選択する確率（サーフェースから見たLocal Lightの見かけの立体角の逆数）  の乗算で計算できます。\nそして、BRDFサンプルの確率は、RAB_GetSurfaceBrdfPdf()で計算されるので、アプリケーション側の処理となりますが、\n DiffuseRayの場合、CosineWeightedのPDF SpecularRayの場合、GGX_VNDFのPDF 上記いずれかをDiffuseProbabilityで選択  したがって、\nDiffuseProbablity * CosineWeightedPDF + (1 - DiffuseProbability) * GGX_VNDF_PDF\nでBRDFサンプルの確率が計算できます。\n1ピクセルあたりで、RISで検討されるライトサンプル数とBRDFサンプル数はDebug UIの設定で決まっていて、numLocalLightSamplesとnumBrdfSamplesに設定されます。このサンプル数を用いて、これらはバランスヒューリスティックで結合されます。これは通常のMulti Importance Samplingと同様の考え方です。\n注意点なのですが、RTXDI_LightBrdfMisWeight()関数の最後では、lightSolidAnglePdfに設定された\u0026quot;ライト上の特定の方向に向けたレイを選択する確率\u0026quot;で除算しています。ここはRTXDIのトリッキーな部分です。あくまで、実際の\u0026quot;sourcePdf\u0026quot;は、この除算の前の値です。 しかし、RTXDIではtargetPdfもlightSolidAnglePdfで除算するので、計算のつじつまが合うようになっています。また、taregetPdfは、シェーディング結果を除算しますが、シェーディング結果もlightSolidAnglePdfで除算されるので、こちらも計算のつじつまが合う仕組みになっています。\ntargetPdfの計算 説明が多少前後しましたが、targetPdfの計算についてです。targetPdfはRISにおいて、積分可能ではないが、理想的なサンプルの確率密度です。(この値は、簡単には積分できず大きさが正規化できないので、PDFと呼ぶべきではなく、単にWeightと呼ぶべきかもしれません。） targetPdfはレンダリングの文脈では、サーフェースがカメラ方向に出すRadianceに比例したレイの分布になるのが一番望ましいです。言い換えれば、カメラの方に最も強く反射される光源へのレイを重点的にサンプリングする分布です。これは、光源のサーフェースでのカメラ方向への反射を計算すればわかります。しかし、光源とサーフェースがVisibleかどうかの判断は、実際にShadow Rayをトレースしなくては分かりません。しかし、これを行えば、実際にレイトレースを行ってシェーディングする処理とまったく変わらなくなり、単にレイのサンプル数を増やすことと同義です。これでは、RISの意味がなくなってしまいまいます。\ntargetPdfの計算では、シェーディングの中で最も処理負荷の高いShadow Rayのテスト処理を省略した値（つまりい光源とサーフェースがVisibleかどうかの判断をせずにシェーディングした結果）が用いられます。\n実際の計算は、RtxdiApplicationBridge.hlsliのRAB_GetLightSampleTargetPdfForSurface()に実装されています。この関数はShadeSurfaceWithLightSample()という関数を呼び出して、シェーディングの計算を行っています。算出された値の輝度値が、そのままtargetPDFとして扱われます。また、blendedSourcePdfの項で説明した通り、シェーディングの計算の最後で、値はlightSolidAnglePdfで除算されます。\nReservoirにサンプルを追加する計算 上記の通り、blendedSourcePdfとtargetPdfの計算が完了すれば、Reservoirにサンプルを追加する処理は簡単です。 RTXDI_StreamSample()に、blendedSourcePdfとtargetPdfを乱数と共に渡して、渡したサンプルが選択された場合は、現在選択中のサンプルの情報を更新します。\nサンプル構築後の処理 numLocalLightSamplesの数だけサンプルを構築し、Reservoirに蓄積した後は、現在Reservoirが選択中のサンプルの情報と、Reservoirに蓄積されたRISの情報のみが残ります。ここまでで複数のサンプルを検討していますが、実際にレイトレース処理は行っていません。しかし、Reservoirには、一番選択するべきサンプルの情報が残っています。\nサンプルを構築するループの直後に、RTXDI_FinalizeResampling()を呼び出しています。ここでの正規化の係数は、1.0/numLocalLightSamplesと思われるかもしれません。しかし実際のプログラムでは、1.0/numMisSamplesで正規化されています。またReservoirのサンプル数Mも1.0に設定しています。これについては後ほど説明します。\nRTXDI_SampleBrdf() この関数は、numBrdfSamplesで指定された数だけ、サーフェースのBRDFをもとにサンプルを構築してReservoir蓄積する処理を行います。\nこの処理は、上記で説明したRTXDI_SampleLocalLights()の処理と対を成す処理です。\n個々のサンプルの構築 まず、RAB_GetSurfaceBrdfSample()を呼び出して、BRDFに基づいたサンプルを構築します。そして、実際にレイトレースを行い、Local Light（Emissive Triangle）にHitするかをテストします。Hitしなかった場合は、このサンプルの処理は終了しReservoirに関する処理は行われません。（しかし、このサンプルがReservoirに蓄積されないというわけではなく、正確にはtargetPDF=0として蓄積された扱いになります。これはReservoirの結合時の処理を見ると判明します。）\n一方でLocal LightにHitした場合は、targetPdfとblendedSourcePdfをそれぞれ計算します。計算は、RTXDI_SampleLocalLights()と全く同じ計算になります。\nサンプルの構築後の処理 ここも、RTXDI_SampleLocalLights()と基本的に同じ計算になります。 サンプル構築のループの直後に、RTXDI_FinalizeResampling()を呼び出しています。ここでの正規化の係数は、Shadow Rayによって棄却されたサンプルを含めるなら、1.0/numBrdfSamplesであるべきと思われるかもしれません。しかし実際のプログラムでは、1.0/numMisSamplesで除算されています。またReservoirのサンプル数Mも1.0に設定しています。これについては後ほど説明します。\nLight SampleとBRDF SampleのReservoirの結合処理 再び、main()の処理に戻ります。RTXDI_SampleLocalLights()によって構築されたlocalReservoirと、RTXDI_SampleBrdf()によって構築された、brdfReservoirを結合する処理を見ていきます。\nRTXDI_CombineReservoirs()の処理 まず、RTXDI_CombineReservoirs()を呼ぶ前に、結合される側のReservoirは、RTXDI_FinalizeResampling()が呼ばれている約束になっています。したがって、結合される側のweightSumは、Finalize前の変数で解釈すると1/targetPDF * 1/M * weightSum に相当する値が設定されています。（ただし1/MはFinalize時に引数で渡す正規化係数）\nこれに、構造体に格納されているMと、引数で渡されたtargetPdfを乗算したものが、risWeightという変数に設定されます。逆算すれば、risWeightは、元のweightSumに(引数の)targetPdf / (構造体に保存されている)targetPdfを乗算したものですから、もしも、1/Mで正規化されていて、targetPdfが同じならば、結局のところ元のweightSumということになります。\nしかし、RTXDI_CombineReservoirs()の引数に渡すtargetPdfは、結合元のReservoirで現在選択されているサンプルの、結合先のReservoirにおけるtargetPdfなので、もしも、結合先でtargetPdfが異なる場合は、その比がweightSumに乗算されることになります。しかし、今回のサンプルプログラムでは、Spatio-TemporalなRISの結合を行わないので、targetPdfは結合の前後で変化しないので、この計算について深く考える必要はありません。\n計算されたrisWeightは、結合されるReservoir全体の、結合先Reservoirにおけるウエイトに相当する値です。\n後は、サンプル数Mを合算し、weightSumにrisWeightを加算して、選択サンプルを乱数で決定することで、Reservoirの結合が完了します。\nlocalReservoir と brdfReservoir の結合 RTXDI_SampleLocalLights()の項で説明したとおり、localReservoirは、1.0/numLocalLightSamplesで除算して正規化するところを、1.0/numMisSamplesで除算したうえに、サンプル数 M を1に設定していました。 これを、RTXDI_CombineReservoirs()の結合される側のReservoirとして処理をすると、risWeightは、Finalize前の変数で解釈すると以下のようになります。\n1/numMisSamples * weightSum\nこの式をわかりやすく書き換えると、以下のようになります。\nnumLocalLightSamples/numMisSamples * 1/numLocalLightSamples * weightSum\nつまり、localReservoirの正規化処理と、localReservoirとbrdfReservoirの、それぞれのサンプル数に基づくバランスヒューリスティックによる結合を同時に処理しているわけです。\n同様に、brdfReservoirの結合時のrisWeightは、\nnumBrdfSamples/numMisSamples * 1/numBrdfSamples * weightSum\nと解釈できます。（ここで、brdfReservoirの生成時に、Shadow RayがMissしてサンプルが破棄されているにも関わらずtargetPdfがゼロのサンプルとして扱われているという解釈ができるわけです。）\n最後に、両者の結合後に、RTXDI_FinalizeResampling()を正規化係数1.0で呼び出していますが、両者の正規化はMISのウエイトによって行われているので、計算のつじつまが合うわけです。\n最後のレイトレースとシェーディング処理 ついに、最終的に採用すべきサンプルが確定し、乗算すべきPDFの算出も完了しました。\nあとはShadow Rayをキャストして、Visibilityを確認すればよいのですが、brdfReservoirのサンプルはその生成過程ですでにShadow Rayを使ってVisibilityを確認しているので、もし、こちらのReservoirからサンプルが採用された場合は、この作業は不要なのでスキップするように処理が書かれています。localReservoir側からからサンプルが選択された場合のみShadow Rayのトレースを行います。\nシェーディング関数のShadeSurfaceWithLightSample()は、RISの過程で何度も呼び出しているので説明不要ですが、ここでもsolidAndlePDFが除算されているので、PDFとの計算のつじつまが合うわけです。 RTXDI_GetReservoirInvPdf()は、既にFinalizeされているReservoirに対して呼び出す関数で、単にweightSumを返します。Finalizeが行われていればそこには、PDFの逆数に相当する値が格納されているはずです。 シェーディングが終われば、TonemappingをかけてUAVに書き出すと、全体の処理が完了します。\nまとめ 最後まで読んじゃった人は「にゃ～ん」ってつぶやいてほしいです。\n","date":1654597828,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654597828,"objectID":"b791aca378a552074170618baabe47ef","permalink":"https://shikihuiku.github.io/post/rtxdi_first_step/","publishdate":"2022-06-07T19:30:28+09:00","relpermalink":"/post/rtxdi_first_step/","section":"post","summary":"最も簡単なサンプルを通じてRTXDIとResampled Importance Sampling(RIS) の基礎を理解する","tags":["RayTracing","Direct3D"],"title":"RTXDIのminimal-sampleを理解する(1)","type":"post"},{"authors":[],"categories":[],"content":"   参考資料   ID3D12GraphicsCommandList::ClearUnorderedAccessViewUint method (d3d12.h)\nhttps://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12graphicscommandlist-clearunorderedaccessviewuint\n  ID3D12GraphicsCommandList::ClearUnorderedAccessViewFloat method (d3d12.h)\nhttps://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12graphicscommandlist-clearunorderedaccessviewfloat\n  なにをするためのものか Texture(RenderTarget)のクリアはRTVを通じて、ClearRenderTargetView()を使う方が効率的です。DepthBufferはDSVを通じてClearDepthStencilView()でクリアする事が強く推奨されます。 では、ClearUnorderedAccessView*メソッドが使われる場合ですが、一般的にはCreateCommittedResource()やCreatePlacedResource()で作成したBuffer(VertexBufferやIndexBuffer、またUAVを通じてアクセスする汎用的なBuffer)をクリアするためのメソッドです。\nSyntax APIインターフェースは以下の様になっています。\nvoid ClearUnorderedAccessViewUint( D3D12_GPU_DESCRIPTOR_HANDLE ViewGPUHandleInCurrentHeap, D3D12_CPU_DESCRIPTOR_HANDLE ViewCPUHandle, ID3D12Resource *pResource, const UINT [4] Values, UINT NumRects, const D3D12_RECT *pRects );  大変手間のかかることに、GPU_DESCRIPTOR_HANDLEとCPU_DESCRIPTOR_HANDLEを指定しなければなりません。当然ながらこれらにはクリア対象リソースの全部又は一部の領域を設定したUAVが正しく記述されている必要があります。 また、このメソッドでクリアしなくてはならないリソースの99.99%は、いわゆる1DBufferで、2次元の概念を保持していませんが、クリアの範囲をRECTで指定する必要があります。また、クリアの際に書き込む値はUINT[4]となっています。 初見では疑問しか沸かないこのAPIインターフェースについて少し考えてみたいと思います。\nなんでUAVが二つもいるの？ このメソッド最大の面倒な点は、CPU側のUAVとGPU側のUAV二つを用意しなくてはならないところです。ちなみにGPU側のDescriptorHeap(つまりShader Visible Descriptor Heap)にも有効なCPU_DESCRIPTOR_HANDLEはありますが、 これをこのメソッドの引数で渡すことはできません。CPU側のDescriptorHeap(つまりShader VisibleではないDescriptor Heap)を用意してUAVを設定して、そのCPU_DESCRIPTOR_HANDLEを引数で渡す必要があります。\nでは、なぜこの二つのUAVが必要なのかというと、このクリア作業のコマンド構築をどのように行うかを考えると少しだけ理解できます。 まず、GPUへのコマンド構築を、対象リソースのアドレス解決を含めて行う場合は、CPU側のDescriptor Heapに設定されたUAVを参照することで、キャッシュの効いた高速なメモリから情報を取得出来ます。 一方で、GPU側にはUAVのアドレスとRECTのみを伝えるシンプルな形でのコマンド構築を行う場合は、GPU側から参照可能なUAVが必要となります。この二つのうちどちらが行われるかは、GPUの実装依存となります。\nしかし、ClearRenderTargetView()やClearDepthStencilView()はGPU側のDescriptorを必要としませんが、クリア作業はGPU側で行われます。 つまり、コマンド構築時にUAVに相当する情報をコマンドバッファに書き込んでいるわけです。ClearUnorderedAccessView*()も同様の作りで問題なかったのではないかと思います。\nRECTでクリア範囲を指定？しかも4要素？ UAV全域をクリアする場合は、RECTによる範囲指定は必要ありませんが、UAVの領域の一部をクリアする場合はRECTを指定する必要があります。RECTは(left, top, right, bottom)を指定する形式になっています。 クリア対象はテクスチャでない場合が多いにも関わらず、RECT指定なのは、単純にClearRenderTargetView()やClearDepthStencilView()のAPIに引きずられたためと思われます。また、RECTの範囲は(left, top) で指定した位置は含みますが、(right, bottom)で指定した位置を含みません。\n例えば、R32UINTの16Byte(つまりR32UINT x 4)のUAVがあるとします。このUAVの先頭8Byteをクリアする場合は、RECTの指定は、(0, 0, 2, 1)となります。また、後半の8Byteをクリアする場合は、(2, 0, 4, 1)となります。 RGBA32UINTの64Byte(つまりRGBA32UINT x 4)のUAVの場合は、同じRECTでそれぞれ先頭32Byte、後半32Byteをクリアする事ができます。 また、クリアに使われる値ですが、R32UINTのリソースにはValue[4]の先頭の要素(Value[0])が繰り返し書き込まれます。RGBA32UINTには、Value[4]の要素が全て書き込まれます。 ほとんどのケースでゼロを書き込むと思いますが、思い通りの値でBufferを埋めたい場合はUAVのFormatとValue[4]に工夫が必要です。\n一体いくつのRECTを指定可能なのでしょうか？ さて、今回これを書こうかなと思った直接の原因についてです。このAPIではRECTの数に上限が無いので、理論上はUINTの上限の個数までRECTが指定可能となっています。しかし実際は、クリア対象のリソースがそこまで大きいものを作成できないでしょう。 また、GPUへのコマンド構築時に何らかの形でRECTの情報を含めないといけませんが、UINTの上限はサイズ的に無理でしょう。今回私は256KByteのリソースにカウンタ記録用の16ByteのUAVを動的に確保するプログラムを作成し、 必要に応じてUAVの提供と、ゼロクリアを行うプログラムを記述しました。1フレームに2000個ほどのカウンタが確保されてクリアが行われました。クリア用にリソース全域を指定したUAVを使ってRECTを2000個設定したところ、 プログラムはこそでクラッシュしました。まぁ、2000は無理かと思いましたので、1023, 511, 255とコマンドを分割しましたがまだクラッシュします。127でクラッシュしなくなりました。私のシステムではこの値が上限値の様です。 さて、私が今書いているプログラムでは一体いくつを上限にするべきなのでしょうか。\n","date":1631018889,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631018889,"objectID":"83ff696a5533cdd5619add5d0389be78","permalink":"https://shikihuiku.github.io/memo/clearunorderedaccessview/","publishdate":"2021-09-07T21:48:09+09:00","relpermalink":"/memo/clearunorderedaccessview/","section":"memo","summary":"Bufferに対して使うときに苦労したのでメモ","tags":[],"title":"ClearUnorderedAccessView*の使い方","type":"memo"},{"authors":[],"categories":[],"content":"はじめに Projection Matrixは何となくややこしいイメージが強い。実際ややこしい。自分でも勘違いすることがある。 なのでいったんまとめることにする。\nRow Major, Column Major, ベクトルとの乗算の順序 Projection Matrixは4x4の正方行列で、メモリに格納するときに行要素を優先して格納すればRow-Major、列要素を優先して格納すればColumn-Majorと呼ばれる。\nRow-Majorは以下の添え字の順番で格納したものを指す。\n$$ \\begin{pmatrix} a_1 \u0026amp; a_2 \u0026amp; a_3 \u0026amp; a_4 \\\\ a_5 \u0026amp; a_6 \u0026amp; a_7 \u0026amp; a_8 \\\\ a_9 \u0026amp; a_{10} \u0026amp; a_{11} \u0026amp; a_{12} \\\\ a_{13} \u0026amp; a_{14} \u0026amp; a_{15} \u0026amp; a_{16} \\end{pmatrix} $$\n対してColumn-Majorは、以下の添え字の順番で格納したものを指す。\n\\begin{pmatrix} a_1 \u0026amp; a_5 \u0026amp; a_9 \u0026amp; a_{13} \\\\ a_2 \u0026amp; a_6 \u0026amp; a_{10} \u0026amp; a_{14} \\\\ a_3 \u0026amp; a_7 \u0026amp; a_{11} \u0026amp; a_{15} \\\\ a_4 \u0026amp; a_8 \u0026amp; a_{12} \u0026amp; a_{16} \\end{pmatrix}\nまた、行列の積は可換ではない。たとえば、4次元ベクトルを行列の右から掛けるか左から掛けるかによって演算が変わるので、これには2通りの演算が存在する。\n$$ \\begin{pmatrix} x^{\\prime} \\\\ y^{\\prime} \\\\ z^{\\prime} \\\\ w^{\\prime} \\end{pmatrix} = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\\\ z \\\\ w \\end{pmatrix} $$\n$$ \\begin{pmatrix} x^{\\prime} \u0026amp; y^{\\prime} \u0026amp; z^{\\prime} \u0026amp; w^{\\prime} \\end{pmatrix} = \\begin{pmatrix} x \u0026amp; y \u0026amp; z \u0026amp; w \\end{pmatrix} \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \\end{pmatrix} $$\nシェーダーを記述する場合は、これらの解釈は実装者に委ねられる。一方で、グラフィックスAPIがこれらの演算を提供する場合もある。 OpenGLのCompatibility Profileでは、Column-Majorでマトリクスをメモリに格納し、Projection Matrixとの乗算はベクトルに対して左側からである。 Direct3D 9では、Row-Majorでマトリクスをメモリに格納し、Projection Matrixとの乗算は、ベクトルに対して右側からである。\n座標変換の過程について 次に座標変換の過程について簡単に説明する。頂点シェーダーが出力する4次元ベクトルは、一般的にはView座標系の位置にProjection Matrixを乗算した結果が出力される。 この座標は同次座標と呼ばれ、W成分で(X,Y,Z)を除算して正規化することで、Normalized Device Coordinate(正規化デバイス座標系)に変換される(Perspective Division)。次にViewport変換を行い、Normalized Device Coordinateを、描画用のバッファ（スクリーン）の領域にマッピングする。 多少の用語の違いがあるが、OpenGL、Vulkan、Direct3Dの3つのグラフィックスAPIは概ね同じ座標変換のステップを持っている。ただし各APIごとに座標軸の考え方や値の範囲が異なるので注意が必要である。\nY軸の反転について 一般的に3D空間上ではY軸を上向きと考える事が多い一方で、2Dスクリーン上では、ピクセルデータを画像の左上から格納する事が多い関係上、Y軸は下向きと考えることが多い。そのため、Projection Matrixによる投影変換、Perspective Division、そしてViewport変換の過程においてY軸を反転させることがある。ここではこれについて説明する。各種変換や用語に関する解説と前後するが、先にここにまとめておく。\nOpenGL OpenGLでは、元来Y軸の反転を行わないという思想の基にAPIが設計されていた。したがって、Viewport変換後のWindow座標系では、画像の左下を原点としてピクセルデータを取り扱う。そのため、Framebufferを画像として表示するときは垂直方向でデータを反転させて表示させるのが一般的である。しかし、現在のOpenGLでは、glClipControl()でGL_UPPER_LEFTを設定すると、Perspective Divisionの際にY軸の符号を反転させる。これによって、Normalized Device CoordinateのY軸の上下が反転するので、Framebufferのデータが画像の左上を原点として格納されるようになる。Perspective Divisionについては、 OpenGL 4.6 Core Pprofileの13.8に記載がある。\nDirect3D Direct3Dの座標変換に関しては、 このドキュメントに記述がある。これによれば、Perspective DivisionはViewport変換のスケーリングの後に行われており、Y軸の符号反転は、Viewportのスケーリングの係数の符号を逆転し、オフセットを調整することで実装されている。 また、 他のドキュメントでも、Normalized Device CoordinateのY軸はView座標系と同じ向きに描写されている。したがって、Direct3DではViewport変換でY軸の符号の反転が行われていると解釈できる。 Viewport変換後のScreen座標系では、画像の左上を原点としてピクセルデータを取り扱う。\nVlukan  Vulkan 1.2によれば、Perspective DivisionでY軸の符号を反転しない。また、Viewport変換時もY軸の符号を反転しない。そして、Viewport変換後のFramebuffer Coordinateの原点は、左上とされている。そのため、VulkanではProjection Matrixの演算でY軸を反転しない限り、Y軸を上向きとする空間を投影変換した像は上下が反転する。 また、Framebuffer Coordinateとの関連性を考えれば、VulkanのNormalized Device CoordinateのY軸は下向きと考えるのが自然である。\nPerspective Division Projection Matrixとの演算を終えた4次元ベクトルは、同次座標を表現する。これを正規化する($w=1$にする）作業は、プログラムなどで制御ができない固定された機能として、グラフィックスAPI側が行う作業となっている。 デフォルトの設定のOpenGL, Vulkan, Direct3Dでは、単純な$w$による除算が行われる。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\end{pmatrix} = \\begin{pmatrix} \\frac{x_v}{w_v} \\\\ \\frac{y_v}{w_v} \\\\ \\frac{z_v}{w_v} \\end{pmatrix} $$\nただし、OpenGLでglClipControl()でGL_UPPER_LEFTが設定されているときは、Perspective Divisionの実行時に$Y$の符号が逆転される。 これは、3D空間上ではY軸を上向きと考えることが一般的である一方、画像フォーマットや、Microsoft Windows や X Window Systemでは、 垂直方向は画面の上から下に向かって座標軸を考えることが多いため、座標軸の向きを入れ替えるための計算である。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\end{pmatrix} = \\begin{pmatrix} \\frac{x_v}{w_v} \\\\ -\\frac{y_v}{w_v} \\\\ \\frac{z_v}{w_v} \\end{pmatrix} $$\n正規化された後の(X,Y,Z)はNormalized Device Coordinate（正規化デバイス座標系）を表現する\nNormalized Device Coordinate (NDC) NDCは、シェーダーコードが出力した同次座標を、Perspective Divitionにより正規化した後の座標系となる。この座標系は$X,Y$は範囲が[-1, 1]と決まっており、 $Z$は[-1, 1]あるいは[0,1]と決まっている。この座標系は、$X,Y$はRenderTargetピクセル位置を表すスクリーン座標系と線形の関係にある。$Z$は深度バッファの値と線形の関係にある。\nOpenGLでは、glClipControl()でNDCのZ軸の範囲を[-1, 1]か[0, 1]のどちらかで選択することができる。デフォルトでは、GL_NEGATIVE_ONE_TO_ONE[-1, 1]が設定されており、GL_ZERO_TO_ONE[0, 1]を設定することで、Direct3D/Vulkanと同じ範囲になる。また、glClipControl()でGL_UPPER_LEFTを設定すると、Perspective DivisionでY軸の符号が反転されるので、NDCのY軸が反転する。\n  NDC   Viewport変換 NDCにおける$X,Y$の値の範囲は[-1, 1]だが、これをViewport変換によりRenderTargetのピクセル位置を表すスクリーン座標系に線形にマッピングする。RnederTarget上でのオフセットと幅と高さを指定する事でViewport変換が実現される。 一般的には、オフセットをゼロに設定し、幅と高さをRenderTargetの幅と高さとすることで、NDCの$X,Y$の[-1, 1]の範囲をRenderTargetの全ピクセルにマッピングすることが多いが、描画領域を分けて複数のViewportのレンダリング結果を一枚のRenderTargetにレンダリングする事もある。\nDirect3DはViewport変換時にY軸の上下が入れ替わるように計算される。\n  Viewport変換   $Z$に関しては、Viewport変換後の深度値の値の範囲を$near, far$の二つの値で指定し、範囲は[0, 1]に収まる様にしなくてはならない。Viewportの$near, far$は深度バッファで使用する値の範囲の事で、 Projection Matrixの$near, far$とは全く意味が異なる。ほとんどの場合では、[0, 1]を指定して、深度バッファが表現できる全ての範囲を使用する。 OpenGLは、NDCのZの範囲を[-1, 1]としているときは、必ずViewport変換時に[near, far]への線形変換が行われる。対して、NDCの範囲が[0, 1]の場合は、Viewport変換の$near, far$が、[0, 1]に設定されている場合は、NDCの$Z$の値がそのまま深度バッファの値として格納される。\n  Viewport変換   右手系、左手系 右手系、左手系とは、単位マトリクスのX,Y,Z軸の各ベクトルの、認識している空間におけるマッピングである。右手系は、右手の（親指,人差し指, 中指）を自然な形で直交させたとき、(X, Y, Z)の向きとなる空間を指す。 左手系も同様である。デフォルトのOpenGLとDirect3DのNDCは、はX軸が画面左から右、Y軸が画面下から上、Z軸が画面手前から奥なので、左手系である。 一方で、glClipControl()でGL_UPPER_LEFTを設定したOpenGLとVulkanのNDCは、X軸が画面左から右、Y軸が画面上から下、Z軸が画面手前から奥なので、右手系である。\nよく耳にする話として、OpenGLが右手系でDirect3Dが左手系という話があるが、OpenGLに関してはglFrustum()/glOrtho()という関数が、 右手系のViewMatrixの-Z方向を、左手系のNDCの+Z方向として変換するためのProjection Matrixを計算することに起因している。 実際にはProjection MatixにはglLoadMatrixで自由に値を設定することができるので、OpenGLは元来シェーダーを使わなくても、右手系でも左手系でも自在に描画できるはずである。 また、Direct3DにはProjection Matrixを計算するAPIは用意されていない。ただし、ユーティリティ関数群のD3DXには、D3DXMatrixPerspectiveRH()という関数が用意されている。 この関数は右手系ViewMatrixの-Z方向を、左手系のNDCの+Z方向として変換するProjection Matrixを計算する。同様に、D3DXMatrixPerspectiveLH()という関数も用意されており、 こちらは、左手系のViewMatrixの+Z方向を、左手系のNDCの+Z方向として変換するProjection Matrixを計算する。\nこのように、ある特定のグラフィックスAPIの座標系が右手系左手系のいずれかに属していると考えること自体が誤りだといえる。\nProjection Matrixの役割 さて、ここからが本題ののProjection Matrixに関する説明になる。View座標系からNDC座標系への変換を担うProjection Matrixには、主に4つの要素がある。\n Y軸の向きの入れ替え (Vulkan) Z軸の向きの入れ替え X,Y軸に関する透視投影変換 Z軸のNDC座標へのマッピング  透視投影変換を行わない正射影というProjection Matrixもあるが、ここでは割愛する。\nY軸の向きの入れ替え (Vulkan) Vulkan特有の事なので一番最初に解説する。Vulkanは先に説明した通り、NDCのY軸は下向きでPerspective DivisionやViewport変換でY軸の符号反転を行わない。 したがって、Y軸が下向きとなる様に頂点シェーダーの出力を行わなければならない。そのため、View座標系でY軸が上向きになるように座標を扱っていた場合、Projection MatrixでY軸を反転させる必要がある。 具体的にはProjection Matrixの、Y成分のスケーリングとオフセットを担当する成分（以下の場合では$a_{22}, a_{23}$）の符号を入れ替える事で、NDCの上下が反転した結果を得る事ができる。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \\\\ a_{21} \u0026amp; -a_{22} \u0026amp; -a_{23} \u0026amp; a_{24} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ a_{41} \u0026amp; a_{42} \u0026amp; a_{43} \u0026amp; a_{44} \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nZ軸の向きの入れ替え 同次座標の$w$は、単にPerspective Divisionでの除算に使われるだけでなく、ポリゴン平面上の属性値補間でPerspective Correctionを行うときに使用されるので、$Z$軸に沿って正しく透視投影変換をするときは下記のどちらかの設定になる。\n $Z_{View}$の正の方向にNDCのZ軸を取る場合は、Projection Matrixを乗算した後の同次座標の$w$に、$Z_{View}$が格納されるようにしなければならない。\nそのため、$Z_{View}$と乗算される位置に$1$を設定する。 $Z_{View}$の負の方向にNDCのZ軸を取る場合は、Projection Matrixを乗算した後の同次座標の$w$に、$-Z_{View}$が格納されるようにしなければならない。\nそのために$Z_{View}$と乗算される位置に$-1$を設定する。  以下は、それぞれ$Z_{View}$正負の方向にNDCのZ軸を設定し、透視投影変換をする場合のProjection Matrixである。殆どのProjection Matrixは下記のいずれかである。 余談だが、この、$w$と乗算される行（あるいは列）は特徴的なので、これを手がかりに、メモリにダンプされたマトリクスが、Row-MajorなのかColumn-Majorなのかを簡単に見分ける事ができる。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$ $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} a_{11} \u0026amp; a_{12} \u0026amp; a_{13} \u0026amp; a_{14} \\\\ a_{21} \u0026amp; a_{22} \u0026amp; a_{23} \u0026amp; a_{24} \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nX,Y軸に関する透視投影変換 透視投影変換は、空間にある物体が視点から離れる程小さく投影される様に変換する役割がある。これにより遠近感が演出される。 視点からの距離が二倍になれば、物体は長さで二分の一の大きさで描画されるようにする。したがって$Z$軸向きに透視投影した$X,Y$座標は、$1/Z$に比例する。\n$X,Y$軸に関する透視投影変換は、つまるところ、以下の式の$a, b, c, d$を決定することにある。 $$ X_{NDC} = \\frac{a * X_{View}}{Z_{View}} + b $$ $$ Y_{NDC} = \\frac{c * Y_{View}}{Z_{View}} + d $$\n$a, c$の値が、水平、垂直視野角を決定し、$b, d$がView座標系からNDC座標系に変換するときのオフセットになる。$b, d$は、View座標のZ軸がNDC座標のX,Yの中心を通る場合はゼロになる。 水平、垂直視野角から$a,c$の値を計算する場合は、視野の両端がNDCにおける[-1, 1]になるように計算すれば良い。   視野角による係数の計算   したがって係数$a,c$は、水平視野角を$\\theta$、垂直視野角を$\\phi$とすれば以下の様に計算できる。（注意：通常は水平視野角と垂直視野角はアスペクト比を通じた線形の関係ではない。通常は水平視野角か垂直視野角のいずれかを基準として正接を計算して、他方はアスペクト比を乗算することで他方の正接を計算するが、ここでは簡便のためそれぞれの視野角を使う。） $$ a = \\frac{1}{tan(\\frac{\\theta}{2})} $$ $$ c = \\frac{1}{tan(\\frac{\\phi}{2})} $$\nもう一つの、係数$a,c$の計算方法として、$Z_{View}=near$平面上での視野の上下左右に相当する$left, right, top, bottom$を指定する方法である。以下の図には$left, right$による水平視野角を示す。   視野角による係数の計算   この場合の係数$a,c$は、$l, r, t, b$の値と、$near$平面までの距離$n$を用いて以下の様に表せる。 $$ a = \\frac{2n}{r - l} $$ $$ c = \\frac{2n}{t-b} $$ また、この指定方法の場合は、$l, r$の値がZ軸で対称でない場合は、オフセットの値が発生する。例として、$l, r$によるオフセットの図を示す。   視野のオフセットの計算   上図はView座標系でのオフセット値になるので、NDC座標系に変換するには、$2/(r-l)$を乗算する必要がある。したがってオフセットの値は以下の様に計算できる。 $$b = -\\frac{r+l}{r-l}$$ $$d = -\\frac{t+b}{t-b}$$\nまた、オフセットがない場合は、$near, left, right, top, bottom$と$\\theta, \\phi$に、以下のような関係が成り立つ。 $$ l = n \\cdot tan(\\frac{\\theta}{2}) $$ $$ r = -n \\cdot tan(\\frac{\\theta}{2}) $$ $$ t = n \\cdot tan(\\frac{\\phi}{2}) $$ $$ b = -n \\cdot tan(\\frac{\\phi}{2}) $$\n次に、Projection Matrixへの各係数の設定だが、$a, c$の値は、それぞれ$X_{View}$, $Y_{View}$と乗算されるように格納する。$Z_{View}$の除算の部分はPerspective Divisionで行われる。 $b, d$の値は、$Z_{View}$と乗算されるようにProjection Matrixに格納する。これは、のちにPerspective Divisionで相殺されることでオフセット値として機能する。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\n以下のマトリクスはD3DXMatrixPerspectiveOffCenterLHが算出する係数と符合する。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{2n}{r - l} \u0026amp; 0 \u0026amp; -\\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; -\\frac{t+b}{t-b} \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\n一方で、Z軸の向きの入れ替えるために、$a_{43}$に$-1$を設定している場合は、$Z_{View}$が乗算される時と、除算される時で符号が異なるため、オフセットの係数の符号が変わる。 以下のマトリクスはD3DXMatrixPerspectiveOffCenterRHが算出する係数や、glFrustum()が算出する係数と符合する。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{2n}{r - l} \u0026amp; 0 \u0026amp; \\frac{r+l}{r-l} \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{2n}{t-b} \u0026amp; \\frac{t+b}{t-b} \u0026amp; 0 \\\\ a_{31} \u0026amp; a_{32} \u0026amp; a_{33} \u0026amp; a_{34} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nZ軸のNDC座標へのマッピング Z軸に関する変換は透視変換ではなく、Z軸の値の一定の範囲をNDCで許されている値の範囲に、大小関係を損なわずに変換することである。通常は、View座標系の広大なZ軸の範囲を、NDCで許されている高々[0, 1]程度の範囲にマッピングする圧縮作業である。 簡単に考えれば、View座標系のZの値にオフセットとスケールを適用すれば実現できるが、これは残念ながら推奨されない。 $$Z_{NDC} = e * Z_{View} + f$$\n 一つ目の理由は、Projection Matrixを使った座標変換による制限によるものである。$X, Y$の値を透視投影変換するためには、同次座標系の$W$の値を$Z$（もしくは$-Z$)の値としなければ、$X,Y$軸に関する透視投影変換が実現できない。そのため$W$の値は決定されていると言える。 この条件では、Projection Matrixとの乗算では、View座標系の$Z$と1次比例の関係を作ることができない。一応ながら、Pixel Shader内で深度バッファに出力する値を直接計算することで実現可能だが、GPUの早期Zカリング機能が無効化されるので実際のアプリケーションの運用では現実的な方法とは言えない。 二つ目の理由は、透視投影変換後のNDCでの$X,Y$平面（つまりはスクリーンスペース）では、$Z_{View}$は線形性を失う。代わりに$1/Z_{View}$が線形性を持つことになる。 投影変換されたポリゴン平面の深度値を高速に計算するならば、線形性を失った$Z_{View}$に比例した式で計算された値は単純な補間では計算出来ず、計算コストが高く効率が良くない。それよりも、大小関係を（反転しつつも）保ちつつ、スクリーンスペースで線形性を持つ$1/Z_{View}$を使う方が合理的だったという経緯がある。 （ちなみに、スクリーンスペースでテクスチャのU,Vなどの頂点属性値は、$attribute/W$と$1/W$をスクリーンスペースで線形補間し、その結果を除算することで補完された頂点属性値を計算している。）  したがって、一般的にGPUでは$Z_{View}$ではなく$1/Z_{View}$を線形変換した結果をNDCのZ座標として採用している。 $$Z_{NDC} = \\frac{e}{Z_{View}} + f$$\nまた、このようにすると、深度バッファに整数の格納フォーマットを使った場合、$Z$の値が小さいときほど、多くのBitを使って表現することになる。 つまり、近くの物体ほど深度バッファの多くのBitが割り当てられるので、これは合理的であるとも考える事ができる。また、$1/Z$の線形変換であれば、Projection Matixで一元的に扱えるのも利点である。\n係数$e, f$の決定は、View座標系における、Z軸の範囲である$near, far$の値が、[0, 1] （もしくは[-1, 1]）になるように連立方程式を解くだけで計算できる。\nNDC[0, 1]の場合 下記の式を解けば、D3DXMatrixPerspectiveFovLHに設定される係数と符合する。\n$$1 = \\frac{e}{far} + f$$ $$0 = \\frac{e}{near} + f$$ $$1 = e (\\frac{1}{far} - \\frac{1}{near})$$ $$e = -\\frac{far \\cdot near}{far-near}$$ $$f = \\frac{far}{far - near}$$\nProjection Matrixに設定するときは、$Z_{View}$と乗算される位置に$f$を設定し、$W$(通常は1.0)と乗算される方に$e$を設定する。Perspective Divisionで$W$(この時点では$Z_{View}$)による除算が行われ、上記の式と等価な計算が行われる。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{far}{far - near} \u0026amp; -\\frac{far \\cdot near}{far-near} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nZ軸の向きを反転させる場合は、Projection Matrixの乗算をよく観察する必要がある。$1/Z_{View}$の係数である$e$は、$W_{View}$と乗算して、$-Z_{View}$で除算される。$W_{View}$は通常$1.0$で$-Z_{View}$も正の数なので、上記で求めた$e$がそのまま使える。 一方で、オフセットの$f$は、$Z_{View}$と乗算して、$-Z_{View}$で除算される。したがって、上記で求めたものの符号を反転させたものを使う必要がある。こうして求めた結果は、D3DXMatrixPerspectiveFovRHの係数と符合する。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{far}{near-far} \u0026amp; \\frac{far \\cdot near}{near -far} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nNDC[-1, 1]の場合 上記と同様の手順で係数$e, f$を求める事ができる\n$$1 = \\frac{e}{far} + f$$ $$-1 = \\frac{e}{near} + f$$ $$2 = e (\\frac{1}{far} - \\frac{1}{near})$$ $$e = -\\frac{2(far \\cdot near)}{far-near}$$ $$f = \\frac{far + near}{far - near}$$\nそれぞれをProjection Matrixに設定すると以下の様になる。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{far + near}{far - near} \u0026amp; -\\frac{2(far \\cdot near)}{far-near}　\\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nZ軸の向きを反転させる場合も先ほどと同様の手順となる。これはglFrustum()関数の係数と符合する。\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -\\frac{far + near}{far - near} \u0026amp; -\\frac{2(far \\cdot near)}{far-near}　\\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nInverse Z 深度バッファに浮動小数点の格納フォーマットが使えるとき、NDCにおける深度のマッピングを、[Near, Far]を[0, 1]ではなく[1, 0]にマッピングすることで、$far$付近での深度バッファの精度不足を解消することができる。 NDCが[-1, 1]の場合や、深度バッファの格納フォーマットが整数表現の場合は、Inverse Zを使う利点はない。 精度については詳しくは以下に解説がある。\nDepth Precision - Nathan Reed\nNDC[1, 0]の場合 Inverse Zの設定は簡単で、先ほどの連立方程式の$near$と$far$を入れ替えて解くだけで係数は求まる。レンダリングの際には、深度バッファのクリア値を、1ではなく0に設定し、ラスタライザーの深度テストの条件を反転させればよい。 $$1 = \\frac{e}{near} + f$$ $$0 = \\frac{e}{far} + f$$ $$e = \\frac{far \\cdot near}{far - near}$$ $$f = -\\frac{near}{far-near}$$\n$$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -\\frac{near}{far-near} \u0026amp; \\frac{far \\cdot near}{far - near} \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$ $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; \\frac{near}{far-near} \u0026amp; \\frac{far \\cdot near}{far - near} \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\nInfinite Far Plane $far$を無限遠に設定する事で、Far Clippingを実質無効化するとともに、浮動小数点の丸め誤差を低減することができる。Projection Matrixに設定する係数の計算は、今まで求めてきた係数の、$far$を無限大で極限を取れば算出される。 Infinite Far Planeのメリットは、非常に遠くのオブジェクトを描画してもクリッピングされることがないことと共に、空や星などを描画する際に、$W_{View}$をゼロとすることで、$(X_{View}, Y_{View}, Z_{View})$方向の無限遠を描画することができることである。\nNDC[0, 1]の場合 $$e = \\lim_{far\\to\\infty} -\\frac{far \\cdot near}{far-near} = -near$$ $$f = \\lim_{far\\to\\infty} \\frac{far}{far - near} = 1$$\nInverse Zを用いないInfinite Far Planeは、無限遠の深度値が1.0となるが、$Z_{View}$が極大化すると正確に描画できないことがあるので注意が必要である。これはProjection Matrixを使った演算とPerspective Divisionでオフセットを設定する場合に、$Z_{View}$による乗算と除算が行われるため、この値が非常に大きな値になれば、浮動小数点数としての精度を失ってしまうからである。\nNDC[1, 0]の場合 一方で、Inverse Zを用いた場合のInfinite Far Planeの係数は以下の様に計算される。 $$e = \\lim_{far\\to\\infty} \\frac{far \\cdot near}{far - near} = near$$ $$f = \\lim_{far\\to\\infty} -\\frac{near}{far-near} = 0$$\nInverse Zを用いたInfinite Far Planeは、オフセットの係数がゼロなので、$Z_{View}$が極大化する事によるProjection Matrixとの乗算による精度の問題を起こさない。以下は、Inverse Zを用いたInfinite Far PlaneのProjection Matrixである。$a_{43}$は$1$でも$-1$でも変わらない。 $$ \\begin{pmatrix} x_d \\\\ y_d \\\\ z_d \\\\ w_d \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{tan(\\frac{\\theta}{2})} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\frac{1}{tan(\\frac{\\phi}{2})} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; near \\\\ 0 \u0026amp; 0 \u0026amp; a_{43} \u0026amp; 0 \\end{pmatrix} \\begin{pmatrix} x_v \\\\ y_v \\\\ z_v \\\\ w_v \\end{pmatrix} $$\n深度バッファから$Z_{View}$の逆算 マルチパスレンダリング等を行っていると、描画された深度バッファより、$Z_{View}$を求めたい時がある。計算自体は単なる逆算なので簡単である。\n深度バッファから$Z_{View}$を逆算するためには、まず、Viewport変換を逆変換して$Z_{NDC}$を計算する必要がある。Viewportの$near, far$とNDCの$Z$軸の範囲が分かれば計算は簡単である。深度バッファとNDCの範囲が一致する場合は、この計算は不要である。 $$Z_{NDC} = \\frac{Depth - near_{Viewport}}{far_{Viewport} - near_{Viewport}}$$\nNDC[0, 1]の場合 Projection Matrixに設定した係数$e, f$を使って$Z_{NDC}$から$Z_{View}$を逆算する。$Z_{NDC}$が正の$Z_{View}$方向ならば以下の式で計算できる。 $$Z_{NDC} = \\frac{e}{Z_{View}} + f$$ $$Z_{View}= \\frac{e}{Z_{NDC} - f} = \\frac{far \\cdot near}{far - Z_{NDC} (far - near)}$$\n$Z_{NDC}$を負の$Z_{View}$方向に取っている場合は符合の操作が必要である。まず、オフセットの値$f$の符合を反転させてあるので、これを反転する必要がある。加えて$Z_{View}$は負の方向なので、最後に符合を反転する必要がある。 $$Z_{View}= - \\frac{e}{Z_{NDC} + f} = -\\frac{far \\cdot near}{far - Z_{NDC} (far - near)}$$\nNDC[1, 0]の場合 Inverse Zを用いた場合は以下の通り。 $$Z_{View}= \\frac{e}{Z_{NDC} - f} = \\frac{far \\cdot near}{near + Z_{NDC} (far - near)}$$ Inverse Zで、$Z_{NDC}$を負の$Z_{View}$方向に取っている場合は以下の通り。 $$Z_{View}= - \\frac{e}{Z_{NDC} + f} = -\\frac{far \\cdot near}{near + Z_{NDC} (far - near)}$$\nInverse Zを用いたInfinite Far Planeの場合は、式はもっと単純になる。ただし、$Z_{NDC}$がゼロの場合はゼロ除算になるので注意が必要である。 $$Z_{View}= \\frac{e}{Z_{NDC}} = \\frac{near}{Z_{NDC}}$$ $Z_{NDC}$を負の$Z_{View}$方向に取っている場合は以下の通り。 $$Z_{View}= -\\frac{e}{Z_{NDC}} = -\\frac{near}{Z_{NDC}}$$\nNDC[-1, 1]の場合 上記と同じ手順で計算する。 Depthから$Z_{NDC}$は以下の通り。 $$Z_{NDC} = \\frac{2(Depth - near_{Viewport})}{far_{Viewport} - near_{Viewport}} -1$$\n$Z_{NDC}$から$Z_{View}$は以下の通り。 $$Z_{View}= \\frac{e}{Z_{NDC} - f} = \\frac{2 \\cdot far \\cdot near}{far + near - Z_{NDC} (far - near)}$$ $Z_{NDC}$を負の$Z_{View}$方向に取っている場合は $$Z_{View}= -\\frac{e}{Z_{NDC} + f} = -\\frac{2 \\cdot far \\cdot near}{far + near - Z_{NDC} (far - near)}$$\n深度バッファからLinear Depthの計算 上記で示した通り、Projection MatrixのNearとFarが分かれば、深度バッファから$Z_{View}$を復元できるが、 実際には$Z_{View}$よりも、単に線形性がある深度値としてのLinear Depthが欲しいケースが多い。 ここでのLinear Depthは[near, far]が[0, 1]にマッピングされており、かつ線形性を保っているものを指す。 計算は先の式の[near, far]を[0, 1]に線形でマッピングするだけである。\nNDC[0, 1]の場合 $$Z_{Linear}= \\{ \\frac{far \\cdot near}{far - Z_{NDC} (far - near)} - near \\} \\frac{1}{far -near} = \\frac{Z_{NDC} \\cdot near}{far - Z_{NDC}(far - near)} = \\frac{Z_{NDC}}{\\frac{far}{near} - Z_{NDC}(\\frac{far}{near}-1)} $$\nNDC[1, 0]の場合 Inverse Zを用いた場合は以下の通り。[near, far]を[0, 1]にマッピングするので、Inverse Zの大小関係は再び反転するので注意。 $$Z_{Linear}= \\{ \\frac{far \\cdot near}{near + Z_{NDC} (far - near)} - near \\} \\frac{1}{far -near} = \\frac{near (1 - Z_{NDC})}{near + Z_{NDC}(far -near)} = \\frac{1 - Z_{NDC}}{ 1 + Z_{NDC}(\\frac{far}{near} - 1)}$$\nRender Targetのピクセル位置から視線ベクトルの逆算 G-Buffer等を用いている場合は、Render Targetのピクセル位置から視線ベクトルを逆算したい事も多い。これも上記と同様で、Projection Matrixからの逆算で計算自体は簡単である。\nRender Targetのピクセル位置から視線ベクトルの逆算するためには、Viewport変換を逆変換して$X_{NDC}, Y_{NDC}$を計算する。 $$X_{NDC} = \\frac{2(X_{Pixel} - OfsX_{Viewport})}{Width_{Viewport}}-1$$ $$Y_{NDC} = \\frac{2(Y_{Pixel} - OfsY_{Viewport})}{Height_{Viewport}}-1$$\nまた、Render Targetのピクセル位置ではなく、フルスクリーン描画したポリゴンのUV値から$X_{NDC}, Y_{NDC}$を逆算する方法も良く用いられる。いずれにせよ、範囲が明確なNDCの座標を再計算するのは簡単である。\n次に$Z_{View}=1$の場合の、$X_{View}, Y_{View}$を計算する。ここでの$a,b,c,d$は、先ほど透視投影変換で求めた値で、$\\theta, \\phi$は水平、垂直視野角である。 $$X_{View1} = \\frac{X_{NDC} - b}{a} = tan(\\frac{\\theta}{2})X_{NDC}$$ $$Y_{View1} = \\frac{Y_{NDC} - d}{c} = tan(\\frac{\\phi}{2})Y_{NDC}$$\n三次元ベクトル$(X_{View1}, Y_{View1}, 1)$は、View座標系の原点からピクセルへのベクトル：視線ベクトルを表すが、長さが1ではないのでライティングの計算をする場合は正規化する必要がある。 一方で、ピクセルのView座標系における位置を求める場合は、このベクトルに$Z_{View}$を乗算することで求める事ができる。\nまとめ もっと簡単にまとめたかったが、ダラダラと長くなってしまった。\n","date":1608995374,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608995374,"objectID":"d99aca88d80373c19929e01e3a073c29","permalink":"https://shikihuiku.github.io/post/projection_matrix/","publishdate":"2020-12-27T00:09:34+09:00","relpermalink":"/post/projection_matrix/","section":"post","summary":"何かとややこしいProjection Matrixに関する解説","tags":["Direct3D","OpenGL","Vulkan"],"title":"Projection Matrixについて","type":"post"},{"authors":[],"categories":[],"content":"NVIDIA Falcor とは D3D12 をバックエンドとした、リアルタイムレンダリングのフレームワークです。 昨今のゲームエンジンほどの手厚い機能はありませんが、レンダリングAPIの抽象化レイヤーが軽量なのでカスタマイズが容易です。 抽象化レイヤーは、ラスタライズのみならず DXR をサポートしているのが大きな特長で、DXR を使って何か試してみたいときには特に有用なフレームワークになります。 また、DepthPre パスや、GBuffer パス、SVGF パスなどが初めから用意されているので、Raster と RT のハイブリッドレンダリングをテストしたい場合や、RT のデノイザー開発にも対応できます。 また、今回は紹介しませんが、CUDA を使った処理もサポートされているので、必要に応じてこちらも使ってみると面白いかもしれません。\nGetting Started 早速ですが、環境をセットアップします。とはいっても、基本的には GitHub のリポジトリを Clone してビルドするだけです。\nhttps://github.com/nvidiagameworks/falcor\n2020/11現在の推奨されるビルド環境は以下の通りです\n Windows 10 version 1809 or newer Visual Studio 2019 Microsoft Windows SDK version 1903 (10.0.18362.1)  また、必須ではありませんが以下のパッケージや機材を準備することをおすすめします\n GeForce RTXシリーズ (もしくはDXRをサポートしたGPU) Windows 10 Graphics Tools (WindowsのOptional Featureからインストールするパッケージ。DebugLayerのために必要) NVAPI (NVAPIによる拡張機能を使用したい場合)  GitのリポジトリをClone git clone --recursive -b master git@github.com:NVIDIAGameWorks/Falcor.git  NVAPIのインストール NVAPIを使用する方は、 ここより NVAPI の最新パッケージをダウンロードして、Externals/.packman/nvapi に配置します。 ダウンロードの際にNVIDIAの開発者登録アカウントが必要になります。 次にFalcorConfig.hを開き、_ENABLE_NVAPI を1に設定します。\n#define _ENABLE_NVAPI 1 // Set this to 1 to enable NVIDIA specific DX extensions. Make sure you have the NVAPI package in your 'Externals' directory. View the readme for more information.  FalcorTest プロジェクトをビルド Tools/FalcorTest をビルドすると、Falcor (Falcor本体。レンダリングのバックエンドになるDLL) と、FalcorTest（各種単機能テストのレンダリングが書かれたアプリ）がビルドされます。 実行すると、各種機能がテストされて結果がコンソールに表示されます。詳細なテストではありませんが、まずここで自分の使いたい機能が正しく動作しているかチェックしておきましょう。 幾つかの機能は無条件にスキップされるようになっているので、適宜ソースを書き換えてテストしましょう。\nたとえば、ShaderModel6.5 のサポートを確認したければ、このように書き換えます。\n#if 0 GPU_TEST(ShaderModel6_4, \u0026quot;Requires shader model 6.4\u0026quot;) { test(ctx, \u0026quot;6_4\u0026quot;); } GPU_TEST(ShaderModel6_5, \u0026quot;Requires shader model 6.5\u0026quot;) { test(ctx, \u0026quot;6_5\u0026quot;); } #else GPU_TEST(ShaderModel6_4) { test(ctx, \u0026quot;6_4\u0026quot;); } GPU_TEST(ShaderModel6_5) { test(ctx, \u0026quot;6_5\u0026quot;); } #endif  TraceRayInline は、2020/11現在は、Falcor内にはシェーダーのコンパイルが通るかのテストコードしかありませんが、とりあえずテストすることが出来ます。\n#if 0 GPU_TEST(testTraceRayInlineAPI, \u0026quot;Requires shader model 6.5\u0026quot;) #else GPU_TEST(testTraceRayInlineAPI) #endif { // We don't actually run the program, just make sure it compiles. ctx.createProgram(\u0026quot;Tests/Slang/TraceRayInline.cs.slang\u0026quot;, \u0026quot;testTraceRayInlineAPI\u0026quot;, Program::DefineList(), Shader::CompilerFlags::None, \u0026quot;6_5\u0026quot;); }  サンプルコードに目を通す ProjectTemplate   Project Template   Samples/ProjectTemplate をビルドして実行すると、ウィンドウが開き、緑の画面とGUIが表示されます。チュートリアルではよくあるお約束の RenderTarget をクリアしているだけのプログラムになります。 プログラム本体はほんの数行で、Guiにボタンを追加するのと画面を緑色でクリアするコードが記述されているだけです。D3D12のAPIは抽象化されており、Render Target のクリアはこの一行で行われます。\n pRenderContext-\u0026gt;clearFbo(pTargetFbo.get(), clearColor, 1.0f, 0, FboAttachmentType::All);  RenderingContext は、メンバーに LowLevelContextData クラスを保持しており、これが D3D12 の CommandAllocator / CommandList / CommandQueue などを保持しています。 D3D12Device はグローバル変数としてアクセス可能で、一通りのAPIの抽象化レイヤーが提供されていますが、ネイティブ API へのアクセスも簡単なものになっています。 一方で、Guiのクラスを覗くと、Dear ImGui がGUIのバックエンドとして使われているのが分かります。\nShaderToy   ShaderToy   Samples/ShaderToy プロジェクトをビルドすると、フルスクリーンの PixelShader のパスで、シェーダー Samples/ShaderToy/Toy.ps.slang が実行されます。 初期化処理の OnLoad で幾つかのステートを作成していますが全部不要な処理です。初期化処理で実際に必要とされる処理は下記の一行のみです。\n// Load shaders mpMainPass = FullScreenPass::create(\u0026quot;Samples/ShaderToy/Toy.ps.slang\u0026quot;);  描画時の onFrameRender では、初期化した mpMainPass のConstant Buffer の値を設定して execute を呼び出して描画しているだけです。\nmpMainPass[\u0026quot;ToyCB\u0026quot;][\u0026quot;iResolution\u0026quot;] = float2(width, height); mpMainPass[\u0026quot;ToyCB\u0026quot;][\u0026quot;iGlobalTime\u0026quot;] = (float)gpFramework-\u0026gt;getGlobalClock().getTime(); mpMainPass-\u0026gt;execute(pRenderContext, pTargetFbo);  このように FullScreenPass クラスを使うと、ポストプロセス処理などで使うフルスクリーンのPixelShaderパスを簡単に構築することができます。Falcor には、FullScreenPass 以外にも極めて基本的な Dispatch や Draw を呼び出す機能がクラスとして標準で用意されています。これらは、Falcor プロジェクトの RenderGraph/BasePasses にあるので、興味があれば参照してください。\nModelViewer Samples/ModelViewerをビルドして実行します。Gui の Load Model をクリックして、Falcor/Media/teapot.obj を読みます。（他にも、Media/Arcade/Arcade.fbxも読み込むことができます） Falcor のアセット読み込みのバックエンドは、 assimp を使用しています。ライトとカメラの設定が無いので、初期状態ではカメラはteapotの中にありますし、レンダリングが真っ黒ですが、ASWDでカメラを動かせば、とりあえずジオメトリが読み込まれていることが分かります。   ModelViewer   ModelViewer.ps.slang をチェックすると、単純にシーン上のライトをイテレーションして、マテリアルを評価しています。したがって、ライトさえ配置されていれば正しくレンダリングされるように書かれています。なので、ModelViewer.cpp の、SceneBuilder がファイルを読み終わった後の箇所で、以下の様にライトの数を確認してライトが無ければ DirectionalLight を追加するように処理を書き加えて実行します。\nif (pBuilder-\u0026gt;getLightCount() == 0) { // no lights. pBuilder-\u0026gt;addLight(DirectionalLight::create()); }  これで、シーン上にライトが無い場合はデフォルトのDirectionalLightが追加されるので、ライティングの様子が見れるようになりました。   ModelViewer with a light   次に、Media/Arcade/Arcade.fscene を読み込んでみます。fscene は、Json で記述された Falcorのシーン格納形式で、ライトとカメラの定義が含まれているので、特にソースコードを変更することなくライティングの様子を見ることができます。Arcade.fscene はリファレンスとして Arcade.fbx と Light.fbx を参照して、内部にライトとカメラの定義を保持しています。   ModelViewer fscene   ModelViewer のプログラムを見てみると、初期化時に WireFrame描画用の RasterStateと、幾つかのCullModeの RasterStatreを作っています。DepthStencilStateも幾つか作っています。これらはGui上で選択して使用する事ができます。 シェーダープログラムは、アプリケーション側は PixelShader のみ指定して、頂点シェーダーと DrawCall の呼び出しは、Falcor 側の mpScene-\u0026gt;render() に任されています。   ModelViewer wire frame   HelloDXR Samples/HelloDXRをビルドして実行します。先ほどの Arcade/Arcade.fscene が起動時から読み込まれています。   Hello DXR (RTX ON)     Hello DXR (RTX OFF)   Ray Traceのチェックボックスで、DXRの有効/無効を切り替える事ができ、影と反射の効果の有無が違いとして見て取れます。 ソースコードを見ると、DXRが無効な時のレンダリングは、Falcorの組み込みレンダリングパスのRasterScenePassが使用されています。設定されているピクセルシェーダーは、ModelViewerとほぼ同じで、Falcorの組み込みシェーディング関数を使用しています。つまり、先の ModelViewer のサンプルプログラムは、RasterState の変更をしないのであれば、RasterScenePassを使って一行で記述できるという事です。\n mpRasterPass = RasterScenePass::create(mpScene, \u0026quot;Samples/HelloDXR/HelloDXR.ps.slang\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;main\u0026quot;); . . . mpRasterPass-\u0026gt;renderScene(pRenderContext, pTargetFbo);  DXR側は、HelloDXR.rt.slangに記述されている各シェーダーをShaderLibraryとしてコンパイルして、RtProgram::Descを使ってHitGroupの定義を行っています。\n RtProgram::Desc rtProgDesc; rtProgDesc.addShaderLibrary(\u0026quot;Samples/HelloDXR/HelloDXR.rt.slang\u0026quot;).setRayGen(\u0026quot;rayGen\u0026quot;); rtProgDesc.addHitGroup(0, \u0026quot;primaryClosestHit\u0026quot;, \u0026quot;primaryAnyHit\u0026quot;).addMiss(0, \u0026quot;primaryMiss\u0026quot;); rtProgDesc.addHitGroup(1, \u0026quot;\u0026quot;, \u0026quot;shadowAnyHit\u0026quot;).addMiss(1, \u0026quot;shadowMiss\u0026quot;); rtProgDesc.addDefines(mpScene-\u0026gt;getSceneDefines()); rtProgDesc.setMaxTraceRecursionDepth(3); // 1 for calling TraceRay from RayGen, 1 for calling it from the primary-ray ClosestHitShader for reflections, 1 for reflection ray tracing a shadow ray mpRaytraceProgram = RtProgram::create(rtProgDesc); mpRtVars = RtProgramVars::create(mpRaytraceProgram, mpScene); mpRaytraceProgram-\u0026gt;setScene(mpScene);  Closest Hitのシェーディングの関数は、Rasterで使用しているものと同一で、prepareShadigData()でサーフェースの情報を取得して、envalMaterial()でシェーディングを行っています。 いる。ただし、ライトの評価の際に checkLightHit() で ShadowRay を飛ばして遮蔽のチェックをしています。また、リフレクションの Ray を飛ばしており、GGXのアルファ値（Roughness相当のパラメーター）で適当にブレンディングしています。（PBRではない） そのため、DXRを有効にした場合は、影と反射が追加されたレンダリングになります。\nMogwaiに目を通す 注意 ビルドの際にコンパイルエラー等は発生しませんでしたが、実行時にPythonのBindingを設定している個所で例外が発生しました。この問題は、VisualStudioをアップデートすることで解決しました。動作確認は Microsoft Visual Studio Professional 2019 - Version 16.7.7 を用いて行いました。同様の問題が発生した場合は、VSのバージョンをチェックすると良いかもしれません。\nMogwai Falcorは Pybind11を使って、RenderGraph を Python で記述するための機構を持っています。Mogwaiはその機構を使ったフレームワークと呼べると思います。 ビルドすると、Pythonで記述されたいくつかの RenderGraph が、実行ファイルの出力先の Data フォルダに用意されます。 起動後に File-\u0026gt;Load Scriptで Data/FowardRenderer.py を読み込みます。これで Foward Rendering の RenderGraph が読み込まれて、使用できるようになります。 次にFile-\u0026gt;Load Sceneで、Falcor/Media/Arcade/Arcade.fsceneを読み込みます。 すると、サンプルの Model Viewerと同様にレンダリングが確認できると思います。しかし、ModelViewerとは異なり、先ほど読み込んだ Pythonのスクリプトに記述された RenderGraph によってレンダリングの動作が定義されています。\nここで、File-\u0026gt;LoadScript で、MinimalPathTracer.py を読み込むことで、パストレーシングのRenderGraphを、追加で読み込むことができます。 HUD上でRenderGraphを切り替えると、MinimalPathTracerにレンダリングパスを切り替えることができます。このように、Mogwaiを使うと、python で記述された複数の RenderGraph を動的に切り替える事ができます。これはレンダリングの比較評価等を行う際に有用だと言えると思います。\nRenderGraph を ForwardRenderer に戻してEditボタンを押すと、RenderGraph の編集ができます。 基本的には FowardRenderer.py に書かれている内容そのままですが、RenderGraph を視覚的に確認してエディットすることもできます。ただ、オマケ的な要素が強く、実際の RenderGraph の構築では、Pythonスクリプトを直接編集したほうが早いです。   Mogwai Render Graph   RenderGraphのノード RenderGraph の個々のノードは、Falcor の RenderPass クラスを継承したクラスです。 たとえば DepthPrePass ノードは DepthPass.dll として事前にビルドされており、これが ForwardRenderer.py の中でインポートされて、他のノードと接続されることでレンダリングパスが構築されています。他にも、GBuffer描画や、CascadedShadowMap, SSAO, Antialias, Tonemapping, SkyBox, それからレイトレーシング用に、MinimalPathTracer, AccumulatePass, SVGFPassなど、他にも幾つかの有用なノードが用意されています。これら RenderGraph のノードは個々のDLLとしてコンパイルされています。そのためのプロジェクトは、Falcor ソリューション内の RenderPasses フォルダに配置されているので確認してみてください。\n  Mogwai Render Graph   DepthPassに目を通す RenderGraph の一つのノードの例として、DepthPass ノードを見てみます。DepthPass ノードは RenderPass クラスを継承して実装され、DepthPass.dllとして配置され、Falcor のレンダリングに使用されます。 ヘッダーファイルを見ると、Gui の描画やリフレクションをサポートする関数、シーン情報にアクセスをするための setScene があります。\n static SharedPtr create(RenderContext* pRenderContext = nullptr, const Dictionary\u0026amp; dict = {}); virtual RenderPassReflection reflect(const CompileData\u0026amp; compileData) override; virtual void execute(RenderContext* pContext, const RenderData\u0026amp; renderData) override; virtual void setScene(RenderContext* pRenderContext, const Scene::SharedPtr\u0026amp; pScene) override; virtual void renderUI(Gui::Widgets\u0026amp; widget) override; virtual Dictionary getScriptingDictionary() override; virtual std::string getDesc() override { return kDesc; }  reflect()を見ると、レンダリングの出力として、2DのDepthStencilを出力することが分かります。入力は無いので、setSceneで設定されたシーンから取得できるカメラとジオメトリを用いてレンダリングすることが想像できます。 DepthBuffer のフォーマットはクラス内部に保持されて、Guiによる設定を介して変更できるようです。その結果がリフレクションの出力ピンに反映されます。\nRenderPassReflection DepthPass::reflect(const CompileData\u0026amp; compileData) { RenderPassReflection reflector; reflector.addOutput(kDepth, \u0026quot;Depth-buffer\u0026quot;).bindFlags(Resource::BindFlags::DepthStencil).format(mDepthFormat).texture2D(0, 0, 0); return reflector; }  描画解像度の設定は無く、DLLの外部で用意されたDepthStencilバッファをBindしてレンダリングする仕組みになっています。シェーダーは、Slangで記述されたピクセルシェーダーが一つだけあります。 中を確認すると、prepareShadingDataを呼び出しています。これは、アルファが完全に透明だった場合に、この関数のなかでDiscardが呼ばれるためです。\nvoid main(VSOut vOut, uint triangleIndex : SV_PrimitiveID) : SV_TARGET { // Calling prepareShadingData() to discard pixels that fail alpha test. The pixel shader has no other side effects. float3 viewDir = normalize(gScene.camera.getPosition() - vOut.posW); prepareShadingData(vOut, triangleIndex, viewDir); }  ちなみに、描画解像度の設定は、Falcor の RenderGraph 側にあります。RenderGraphCompiler::allocateResources() が RenderGraph の変更や描画解像度変更をトリガーとして、使用されている出力ピンのリソースを ResourceCache に登録します。その時に、各種 RnederTarget の解像度の設定が行われます。\n実際に RenderGraph のノードを実装するならば、ぜひ Docs/ Tutorials をご一読する事をお勧めします。\n最後に ビルドして実行するだけではいまいち概要がつかみにくい Falcor ですが、リポジトリの ドキュメントを読めば、この記事に書いてあることを含めて記載があります。ソースコードの規模もあまり大きくないので、全体の把握も比較的容易だと思います。アセットもFBX形式が読み込めるので、手元のアセットでテストしたい場合なども比較的短時間でセットアップできるのではないでしょうか。今回は紹介していませんが、 ORCAをはじめ、CC-BY等のオープンライセンスの元で公開されているアセットなどもあるので、これらを活用してレンダリングのテストを行う事も可能だと思います。\n今回は、簡単にですが Falcor を触ってみました。この手の軽量なレンダリングフレームワークというのは、自作含めて多数あると思いますが、RasterとDXRをシームレスにサポートしている軽量フレームワークはあまり見かけません。 そういう意味ではユニークな存在かもしれません。\n","date":1604801571,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604801571,"objectID":"b23e5eafb2462b2afeefae474c42d193","permalink":"https://shikihuiku.github.io/post/falcor_getting_started/","publishdate":"2020-11-08T11:12:51+09:00","relpermalink":"/post/falcor_getting_started/","section":"post","summary":"Raster / Realtime Raytracing の Rapid Prototyping に使えるかも","tags":["DX12"],"title":"NVIDIA Falcor を使ってみる","type":"post"},{"authors":[],"categories":[],"content":"参考資料   Importance Resampling for Global Illumination\nhttps://scholarsarchive.byu.edu/cgi/viewcontent.cgi?article=1662\u0026context=etd\n  Rendering Millions of Dynamic Lights in Real-Time\nhttps://news.developer.nvidia.com/rendering-millions-of-dynamics-lights-in-realtime/\n  Resampled Importance Sampling Importance Resamplingの概要  確率分布$p$に基づいたM個($(M \\ge 1$)のサンプル、$(X = \\langle X_1, X_2, \u0026hellip; , X_M \\rangle $を生成する 各サンプルのウエイト$ｗ_j$を計算する $X$のなかから、一つのサンプル$Y$を、ウエイト$\\langle w_1, w_2, \u0026hellip; , w_M \\rangle$に基づいて選ぶ  もし、$w_j = \\frac{g(X_j)}{p(X_j)}$としたならば、サンプル$Y$は、おおよそ$g$に基づいて分布する。 上記のリサンプリングの効果とは、密度$p$のソースのから、サンプルを取り出し、\u0026ldquo;filter\u0026quot;することで、 結果として、サンプル$Y$がおおよそ$g$の分布になるようにすることである。\nこれを図解した下記の例は、8個のサンプルを生成して、それぞれのウエイトを計算した場合のイメージになる。       サンプル数である$M$を分布の補間変数としてとらえることができる。 $M=1$のとき、$Y$は$p$に従って分布する。$M\\rightarrow\\infty$とすると、$Y$の分布は$g$に近づく。 一般的には、$M$が有限個である事によって導入されるバイアスを無視できる程度に小さくするためには、$M$は非常に大きくなけれならない。\n注釈：サンプル列$X$から、あるサンプル$Y$が選択される確率は $w_i / w_{sum}$で計算できるが、積分の範囲に対して、そのサンプルが選択される確率を考えると、 これに$M \\cdot p(X_i)$を乗算する必要がある。したがって、そのサンプルが選択される確率は$M \\cdot g(X_i) / w_{sum} $となる。これはそのまま次項でのImportance Samplingの確率となっている。また、$M / w_{sum} $は、サンプル列$X$において一定なので、$g(X_i)$に比例する関数で、選択が行われている事になる。\nResampled Importance Samplingの概要 Importance Resamplingの考え方をImporntace Samplingに導入したものをResampled Importance Sampling（RIS)と呼ぶ。\n下記の積分を求めたいとする。 $$ I = \\int_{\\Omega} f(x) d\\mu(x) $$\n二つの確率密度関数$p$と、確率に即した値を返す関数$g$を導入する。 $p$は、比較的簡単に計算でき、正規化することができるが、上記積分をImportance Samplingするのにあまり良い分布となっていないとする。 $g$は、良い分布特性を持っているが、計算が複雑で正規化されていない、またはするのが困難だとする。 通常のImportance Samplingでは、$p$を使う事しかできないが、RISを使うことで、$g$の分布特性をunbiasedな条件下で使うことができる。 $X$(Resamplingによって生成されたサンプル列)および$Y$(Xからウエイトに基づいて乱択されたサンプル)がImporntace Resamplingを用いて導出された場合の、RISのEstimatorは以下の通りとなる。\n$$ \\hat I_{ris} = \\frac{1}{N} \\sum^{N}_{i=1} w(X_i, Y_i) \\frac{f(Y_i)}{g(Y_i)} $$\nウエイト関数$w$は、$g$が正規化されていないことと、$Y$の分布が$g$を近似するようにする必要があることの二つを考慮して選択されなければならない。 そのための適切なウエイト関数$w$は実は非常に単純で、リサンプリングステップで計算された重みの平均である。\n$$ w(X_i, Y_i) = \\frac{1}{M} \\sum^{M}_ {j=1} w_ {ij} = \\frac{1}{M} \\sum^{M}_ {j=1} \\frac{g(X_{ij})}{p(X_{ij})}$$\n注釈：$X_i$, $Y_i$はそれぞれが、Importance Samplingの$i$番目の試行の時に作られた、M個のサンプル列$X_i$と、そこから選択された一つのサンプル$Y_i$を指す。 $w_{ij}$は、Importance Samplingの$i$番目の試行の時に作られた、M個のサンプル集合の、$j$番目のサンプルのウエイトを指す。\n注釈２：ウエイトは、結局のところ、二つの関数$p(x)$と$g(x)$のプロファイル（関数の形状および大きさ）の比になる。(例えば、$g(x)$が$p(x)$に対して、平均で2倍の値を持つ場合は、 $g(Y_i)$で除算すると、推定値が期待値の$1/2$に収束してしまう。$w$はその補正として2倍の補正を掛ける役割を担う。)\n上記の2式を合わせると下記の通りになる。 $$ \\hat I_{ris} = \\frac{1}{N} \\sum^{N}_ {i=1} \\left( \\frac{f(Y_i)}{g(Y_i)} \\cdot \\frac{1}{M} \\sum^{M}_ {j=1} \\frac{g(X_{ij})}{p(X_{ij})} \\right) $$\n$M=1$のときは、RISは$p$によるImportance Samplingになる。RISがunbiasedである条件は、$f$が非ゼロの領域では、$p, q$共に非ゼロである必要があるのと、$M,N$共にゼロより大きい必要がある。 RISがunbiasとすると、誤差は分散に起因するもののみとなる。ターゲット分布関数$g$と共に、パラメーター$M,N$を適切に選ぶことが分散の低減につながる。\nレンダリングにおいて、ライティングの関数を$f=F_s GVL_e$とするとき、ターゲット分布関数$g$を、$g=F_s GL_e$とすることができる。レイのトラバースが必要な$V$項を取り除いたものである。正規化されている必要がないのでそのまま使用できる。\n次は、適切な$M,N$の設定である。\nRobust Approximations of M and N 詳細は割愛するが、最適なM,Nを選ぶための計算には、実際のモンテカルロ法の試行が必要になる。しかし実際にはモンテカルロ法の試行の前に知りたい。 ここではサンプルの生成や実際の推定値の計算にかかる時間を基にして、$M,N$の導出を行う。 $T_x$を$p$に基づいてサンプルを生成してウエイトを計算するのにかかる時間とし、$T_y$をIRS Estimatorで推定値を計算する時間とすると、全体の計算時間は以下の様になる。 $$ T=MNT_x + N(r(M) + T_y) $$ $r$はリサンプリングにかかる時間で、離散的な分布からサンプルを選ぶ作業は高度に最適化されており、時間は無視できるとすると以下の様になる。 $$ T=MNT_x + NT_y　$$\nここで、リサンプリングの各ステップに同じ計算時間を割り当てるとすると、以下の様になる。 $$MNT_x = NT_y $$\nこの条件下では$M$は以下の様に求まる。 $$M=\\frac{T_y}{T_x} $$\n注釈：$N$は上式から求まらないが、総計算時間の関数として計算することができる。$M$は、$T_y$($V$項を含んだシェーディングの評価)が、他の項の計算に比べてどのくらい時間がかかるかによって 決定されるといえると思う。時間がかかればかかるほど$M$を大きくした方が良いという事になる。\nレンダリングするシーンの複雑度が均一であれば、この値は事前計算することができる。$T_x, T_y$を得るために数千のPrimaryRayをキャストして計算する。\n 参照論文  のレンダリングの例では、二つのポリゴンライトと環境マップを用いてライティングしている。この場合RISは同じ計算量で分散を70%低減できた。しかし、ライティングから環境マップを取り除くと、分散を10%しか低減出来なかった。 これは、$L_e$の成分にあった環境マップが分散を導入していたからで、これはターゲット分布関数$g$のウエイトとして計算されていた。(つまり$L_e$が導入した分散に効果的に作用した。) しかし、これが取り除かれると、$V$項による分散が支配的となり、通常のImportance Samplingとの差が得られなくなった。\nWeighted Reservoir Sampling 上記Resampled Importance Samplingは、計算時に$M$個のサンプルストリームを用意する必要があった。これを、実際にストレージを確保して保存することなく実現するための手法がWeighted Reservoir Samplingになる。 WRSではサンプルストリームに値を追加するたびにDraw（分布に基づく乱択）を行う代わりに、ストリームに何個要素を追加しても、ウエイトの合計値である$w_ {sum}$と現在の出力である$y$のみ記録することでRISと同等の機能を実現する。\nWeighted Reservoir Samplingの手法 以下はWRSのpuseudo codeである。サンプルストリームの要素の追加(update)のたびに、$w_ {sum}$を更新しつつ乱択を行う。Reservoirは常に選択候補となる一つのサンプルを保持する形となる。同じストリームから複数のサンプルを乱択することは出来ないが、必要なストレージが非常に少ないのが特徴。\nclass Reservoir y ← 0 // The output sample w ← 0 // The weight for the output stream wsum ← 0 // The sum of weights M ← 0 // The number of samples seen so far function p(x) return probability of x function g(x) return target distribution of x function update(xi, wi, cnt) wsum ← wsum + wi M ← M + cnt if rand() \u0026lt; (wi /wsum) then y ← xi function reservoirSampling() Reservoir r for i ← 1 to StreamLen do generate xi r.update(xi, g(xi)/p(xi), 1) w ← (1 / g(y))(wsum / M) return y, w  Combine Multiple Reservoirs WRSの特長として、個々のストリームにアクセスすることなく、生成された複数のストリームを結合することができることが挙げられる。結合の方法は以下の通りで新しいReservoirに、結合するReservoirを入力することで行う。 s.update()の第二引数のウエイトは、結合する際のReservoirのターゲット分布関数$g$が同じ場合は、r.wsumと等価である。しかし、結合する際のReservoirのターゲット分布関数が異なる場合、 $g(y)/g\u0026rsquo;(y)$をr.wsumに乗算する必要がある。 これはそのための計算である。\nfunction combineReservoirs(g, r1, r2, . . . , rk) Reservoir s // a new reservor foreach r ∈ {r1, . . . , rk } do s.update(r.y, g(r.y) · r.w · r.M, r.M) w ← (1 / g(s.y))(s.wsum / s.M) return s.y, w  注釈１：ウエイトのスケーリング(g(r.y) · r.w · r.M)について。\n結合元の$w$は、 $$\\frac{1}{g\u0026rsquo;(y\u0026rsquo;)} \\frac{1}{M\u0026rsquo;} \\sum^{M\u0026rsquo;}_ {i=1} \\frac{g\u0026rsquo;(X_i)}{p\u0026rsquo;(X_i)} $$ なのだが、これは通常のImportance Samplingにおける確率密度関数の逆数に相当するものでなくてはならない。（通常1/p(x)で表すが本式のpとは意味が異なる。） したがって、これに$g(y\u0026rsquo;)$ を乗算すると、以下の様になり、これは、通常のRISの一つのウエイトである、$g / p$と同義になる。（ただし、この場合のベースの確率密度関数（先ほどpと呼んでいたものに相当するもの）が実際にどんなプロファイルかは知らない） $$\\frac{g(y\u0026rsquo;)}{g\u0026rsquo;(y\u0026rsquo;)} \\frac{1}{M\u0026rsquo;} \\sum^{M\u0026rsquo;}{i=1} \\frac{g\u0026rsquo;(X_i)}{p\u0026rsquo;(X_i)} $$ そして、この$y\u0026rsquo;$は、$M\u0026rsquo;$個のサンプルの中から選択されたサンプルなので、加重平均で確率計算するために、その分のウエイトに相当する$M\u0026rsquo;$を乗算する。 $$\\frac{g(y\u0026rsquo;)}{g\u0026rsquo;(y\u0026rsquo;)} \\sum^{1}{M\u0026rsquo;} \\frac{g\u0026rsquo;(X_i)}{p\u0026rsquo;(X_i)} $$ このように、ウエイトのスケーリングを解釈する事ができる。\nまた、結合後のウエイトの計算部分((1 / g(s.y))(s.wsum / s.M))は、まず、(s.wsum / s.M)の部分は、全てのサンプルのg/pの平均値になり、関数pとgのプロファイルの比と考える事ができる。 また、この値は、全てのサンプルのg/pの平均値なので、候補となった全サンプルにおいて一定の値と解釈でき、最終的な確率密度の逆数は、1/gに比例していると考える事ができる。\n注釈２：Biasについて。\n実際には、結合先と結合元で、確率密度関数や、ターゲット分布関数が異なれば、もとから一つのサンプル集合だった場合とは異なる演算になる。ただ、これによってBiasが導入されるかは別である。 実際は、結合結果として選択されたサンプルが、結合元の確率密度関数でゼロになる場合にBiasが導入されることになる。この場合は、無効なウエイト値が計算に入るので、実際の値より小さくなる方向にBiasがかかるはずである。 それ以外では、結果的に分散を大きくするかもしれないが、Biasが導入されることはない。\nNaive Unbiased combine UnbiasedなWRSの結合をするためには、結合前のターゲット分布関数もしくは、確率密度関数が必要になる。（正確にはその両方。p(), g()の両方が非ゼロである事が確認できる必要がある） Reservoirを結合してサンプルの選択をするところまでは同じだが、最後に、選択されたサンプルが、結合前のターゲット分布関数で有効であるかどうかを確認して、無効であればウエイトから除外するようにしている。\nfunction combineReservoirsUnbiased(g, r1, r2, . . . , rk, g1, g2, . . . , gk) Reservoir s // a new reservor s.combineReservoirs(g, r1, r2, . . . , rk) Z ← 0 foreach gi ∈ {g1, . . . , gk } do if gi(s.y) \u0026gt; 0 then Z ← Z + ri.M w ← (1 / g(s.y))(s.wsum / Z) return s.y, w  なぜこれでUnbiasedになるか 一見すると、$\\frac{w_{sum}}{M}$を乗算する代わりに、$\\frac{w_{sum}}{Z}$を乗算するが、一方で$w_{sum}$の計算時には、結合前のターゲット分布関数の値に関わらず加算をしているので 矛盾しているように見える。ここで簡単な例を使って考えてみる。\n ReSTIR のFig7で使われている$p(), g()$を例として、$p_1()$で作られた一つの$M_1$個のサンプルのReservoirと、$p_2()$で作られた$M_2$個のReservoir を結合する場合を考える。$g(), p1(), p2()$はそれぞれ以下の通りで、$p_2()$は0.5を超えるとゼロになる。\n $ g(x) = 2 -2x$, $ p_1(x) = 1$ $ g(x) = 2 -2x$, $ p_2(x) = 2H(1/2 -x) $(p2(x)=0 if x \u0026gt; 0.5 otherwise =2)  ここで、p1側のReservoirが、0.6のサンプルを選択していて、p2側のReservoirが0.3のサンプルを選択している状態だとする。この二つを結合した結果、0.6のサンプルが選択された場合のBiased 結合によるウエイトは以下の様に計算される。 $$\\frac{1}{g(0.6)} \\frac{g(0.6) · w_1 · M_1 + g(0.3) · w_2 · M_2}{M_1 + M_2} $$ ここで、さらに簡単にするため、$M_1=M_2=1$とすると、上記の式は以下の様になる。 $$\\frac{1}{g(0.6)} \\frac{g(0.6)/p_1(0.6) + g(0.3)/p_2(0.3)}{2} $$ 一方で、最初の定義より、$p_2(0.6)$がゼロである事が分かっているので、p2側のReservoirから0.6のサンプルが生成されることは無い。ここで改めて、0.6のサンプル発生からの結合の過程を通じて、最終的に0.6のサンプルが選択される確率考えると、p1側のReservoirに0.6というサンプルが発生する確率と、結合したときにp1側が選択される同時確率で表す事ができる。 $$ p1(0.6) \\frac{g(0.6)/p_1(0.6)}{g(0.6)/p_1(0.6) + g(0.3)/p_2(0.3)} $$ この式より分かることは、$M_1+M_2=2$で除算しているBiased 結合は実際の発生確率の逆数よりも小さい値になることが分かる。そしてそれをUnbiased にするには、2では無く1、すなわち$M$ではなく$Z$で除算するのが正しい事が分かる。 念のため、$M_1$と$M_2$に対する条件を自由した場合に、$p_1$側が選択される確率は以下の様に表す事ができる。 $$ \\frac{1}{w_1} \\frac{g(0.6) · w_1 · M_1}{g(0.6) · w_1 · M1 + g(0.3) · w_2 · M_2} $$\nここからも、$M$ではなく$Z$で除算することでBiasが排除できる事が分かる。\n結合時の安定性について  ReSTIR のFig7では、結合時の数値安定性についても言及している。 先の$p_2$を以下の様に変更し、全域で発生確率が非ゼロになるが、0.5を超えると非常に小さい確率となるように変更している。\n$ p_2(x) = max(1.9\u0026hellip;H(1/2 -x), 10^{-4}) $(p2(x)=10^{-4} if x \u0026gt; 0.5 otherwise =1.9\u0026hellip;)\nこのようにした場合に、先の単純な例を用いて、$p_1$側から0.3、$p_2$側から0.6、そして結合による選択で0.6が選択された場合のウエイトは以下の様に算出される。 $$\\frac{1}{g(0.6)} \\frac{g(0.3)/p_1(0.3) + g(0.6)/p_2(0.6)}{2} $$ 一方で、p1側から0.6、p2側から0.3、そして結合による選択で0.6が選択された場合のウエイトは以下の様に算出される。 $$\\frac{1}{g(0.6)} \\frac{g(0.6)/p_1(0.6) + g(0.3)/p_2(0.3)}{2} $$ 先の通り、$p_2(0.6)$は極小なので、$g(0.6)/p_2(0.6)$は極大化する。両者はどちらも0.6を出力するが、サンプルが発生した経緯でウエイトが大きく異なるうえに、$p_2$側は極稀にしか発生しない。 安定した値に収束させるには非常に数多くの、結合後のサンプルが必要となる。\nMIS Combineについて  ReSTIR の4.3項では、MISを使った結合に言及している。 これも、先ほどUnbiasedの説明をした例を使って考える。以下の式は、$p_1$側から0.6が生成されて結合した際に0.6が選択される確率と、$p_2$側から0.6が生成されて0.6選択される場合の確率である。 選択されない側のサンプルは単にｘとしているがこれにあまり深い意味はない。 $$ \\frac{1}{w_1} \\frac{g(0.6) · w_1 · M_1}{g(0.6) · w_1 · M1 + g(x) · w_2 · M_2} $$ $$ \\frac{1}{w_2} \\frac{g(0.6) · w_2 · M_2}{g(x) · w_1 · M1 + g(0.6) · w_2 · M_2} $$ 端的に言えば、$p_1, p_2$を結合した後に0.6が出力されるのは上記2つのうちのいずれかである。また、結合した結果$p_1$側を選択した場合に算出されるウエイトは以下の通りだが、 $$\\frac{1}{g(0.6)} \\frac{g(0.6) · w_1 · M_1 + g(x) · w_2 · M_2}{M_1 + M_2} $$ これを少し変形すると以下の様に書ける。 $$\\frac{M_1}{M_1 + M_2} \\frac{1}{g(0.6) · M_1} \\{g(0.6) · w_1 · M_1 + g(x) · w_2 · M_2\\} $$ このとき、先頭の項にある$\\frac{M_1}{M_1 + M_2}$を$p_1$側のサンプリング戦略のMISウエイトと見做すことができる。そして$p_2$側も同様に考える事ができ、MISウエイトの合計値が1なので、Biasが発生することもない。 つまり、結合時のMISウエイトは、各サンプリング戦略のサンプル数の加重平均であると言える。また、MISウエイトは、その値のサンプルを出力する可能性のあるサンプル戦略のウエイトの合計値が1になるのであれば、どのようなウエイトでもBiasを導入することは無いことが知られている。これを利用して、先の項で説明した結合時の安定性を改善する事ができる。例えば論文にある通り、各戦略での初期サンプルの発生確率の加重平均にする事もできる。 ここで、$x_s$は最終的に選択されたサンプルの値で、$S$はサンプリング戦略数。$p_s()$は各サンプリング戦略の初期サンプル発生確率である。 $$ \\frac{p_s(x_s)}{\\sum^{S}_{i=1}p_i(x_s)} $$ このようにすれば、先の$p_2(0.6)$のような極小の確率で発生するサンプルがあった場合に、最終的なImportanceWeightの極大化を防ぐ事ができる。\n閑話休題 ReSTIR のFig7について まずは、グラフ(a)(b)について。 被積分関数については言及がないが$ f(x) = 2-2x$と仮定する。\n $ g(x) = 2 -2x$, $ p_1(x) = 1$ $ g(x) = 2 -2x$, $ p_2(x) = 2H(1/2 -x) $(p2(x)=0 if x \u0026gt; 0.5 otherwise =2)  としている。\n$1/p(y)$をVisualizeしているといってるが、この$1/p(y)$はRISのEffectivePDFの逆数、つまり、$\\frac{1}{g(y)} \\frac{1}{M} \\sum^{M}_ {i=1} \\frac{g(X_i)}{p(X_i)} $を指していると思われる。\n  一つは、サンプルのヒストグラムの逆数をプロットしたもの\nImportance Samplingを、$f(x)$に沿って行えたとするならば、生成されたサンプルのヒストグラムは、$f(x)$に比例しなくてはならない。 したがって、サンプルのヒストグラムを正規化して逆数をとれば、$1/f(x)$に沿うと考えられる。このグラフは、サンプル数に基づいて、ヒストグラムを正規化して逆数にしたグラフと思われる。 $p_2$が0.5以上のサンプルを生成しないので、そこに大きな境界が出来ている。（実際の運用なら単純によろしくない。)　0.5以上は、$p_1$の方で生成されるが、こちらのサンプル確率は単純に$g(x)$(つまりf(x))に比例しているため、 1に近づくにつれてサンプルの生成される確率が低下している。それに伴ってヒストグラムの乱れが顕著に見られる様になっていると思われる。また、0.5以下ではサンプル数が多い傾向があり、0.5以上ではサンプル数が少ない傾向があるが、これは$p_2$が偏った範囲でサンプルを生成することによる影響と思われる。\n  もう一つは、RISのウエイトの平均値\nこちらは単純に先のRISのウエイト（実質的なサンプルの確率としてのウエイト。Effective PDF）の逆数の平均だから解釈は簡単。サンプルの出現確率と、RISのウエイト値は、統計的に一致していない場合は、真値からのBiasが生じる事になる。 グラフ(a)は、0.5以上で、明らかな乖離が見られる。サンプルを発生させない$p_2$のウエイトを算入しているためBiasが発生しているのが分かる。\n  次に、グラフ(c)(d)について。 $ p_2(x) = max(1.9\u0026hellip;H(1/2 -x), 10^{-4}) $ (p2(x)=10^{-4} if x \u0026gt; 0.5 otherwise =1.9\u0026hellip;)\nに変更し、$p_2$で非常に低確率で0.5以上のサンプルが生成されるように変更したもの。\n  0.5以上のサンプルが生成されるとき、$p_2=10^{-4}$なので、$\\frac{g(X_i)}{p(X_i)} $の部分は値が非常に大きくなると思われる。すると、$p_2$内でのResamplingで選択される可能性が高く、 また、$p_2$の$w_{sum}$も大きくなることから、$p_1,p_2$を結合する時に選択される可能性が高くなる。 また結合後の$w_{sum}$への影響も大きいので、選択の可否に関わらず、$p_2$側で、0.5以上のサンプルが生成されることにより、最終的なウエイト値に大きな乱れを導入する原因になっている。(c)\n  MIS（おそらく初期サンプルの発生確率の加重平均）で結合する場合、各サンプリング戦略（この場合は、結合対象の各RISのこと。いわゆる$p_1,p_2$）における初期サンプルの確率が計算出来なくてはならない。しかし、RISでは、自身のサンプル戦略の結果として出たサンプルのウエイト値（サンプルの確率）は計算されるるが、任意のサンプルの確率を計算する術がない。しかしここでは何らかのサンプルの確率に相当するものが計算できるものとして話を進める。\n$p_2$で非常に低確率な0.5以上のサンプルが出た場合は、RISのウエイトは極大化するはずである。しかしMISウエイトを初期サンプルの発生確率の加重平均にすれば、極小化した出現確率による除算は高確率で相殺することができる。(d)\n  ReSTIRのSupplementary Filesの方に記載があるMISに関する議論  こちら のSupplementary Documentでは、RISを結合する際のMISのウエイトの計算方法について 言及している。この手法にとってとても重要な部分なのだが、とにかくこちらに記載がある。\n二つの手法が紹介され、一つはTalbot et alの手法と、本論文の提案手法である、Bitterli et alの手法を比較している。質の良いMISウエイトを計算するには、任意のサンプルの、個々のサンプル戦略に於けるPDFが計算できる必要があるが、RISは、自身が選出したサンプルのウエイト値（PDFの逆数に相当する値）を計算することしかできない。そこで、PDFの代替となる値を計算する必要がある。重要なのは、この値はMISのウエイトを計算するためのもので、各サンプリング戦略(各RIS)におけるPDFと必ずしも一致する必要がない。 ただ、unbiasedなウエイトを算出するだけなら、\n あるサンプルに於ける各サンプリング戦略のウエイトの合計値が1 ある戦略で、あるサンプルのPDFがゼロならば、そのウエイトはゼロ の二つを守れば良い。  Talbot et alの方法は、各RISを結合の際に、結合対象サンプルを、全てのRISのターゲット分布関数:$g$で評価してウエイト$g_i / g_{sum}$を計算し、正規化（$m$の平均が1.0になるように）して、これをサンプルのウエイト値に乗算して結合している。直感的で分かりやすい。上記の項で説明したMIS Combineというよりは、結合時の各戦略の選択確率のモジュレーションと解釈した方が適切だと思われる。 ターゲット分布関数:$g$の評価回数が2次的に増大するのがこの方法の大きな欠点である。\nfunction combineReservoirsTalbotMIS(g, r1, r2, . . . , rk, g1, g2, . . . , gk) Reservoir s // a new reservor foreach r_i ∈ {r1, . . . , rk } do // calculate normalized MIS weight using g() g_sum ← 0 foreach g_j ∈ {g1, . . . , gk } do g_sum ← g_sum + g_j(r.y) m ← g_i(r.y) / (g_sum / k) s.update(r.y, g(r.y) · r.w · r.M · m, r.M) w ← (1 / g(s.y))(s.wsum / s.M) return s.y, w  本論文の提案手法であるBitterli et alの手法では、結合して、最終的に選択されたサンプルを使って、各戦略のターゲット分布関数の加重平均でMISウエイトを計算している。 最終的に$1/M$の代わりにこれを乗算している。この手法では、ターゲット分布関数の評価回数はサンプリング戦略数（結合するRISの数）なので、 Talbot et alの方法に比べたら、明らかに計算量が少ない。\n注釈：この式はまだ理解できてない。例えば一つの戦略で、Mに非常に大きな数を使った場合、s.wsumはそれに準じて大きな数になる。 一方でmは、ターゲット分布関数に変化が無い場合、サンプリング戦略数が増えればそれに準じて小さな値になっていくはずである。 これら両者は直交したパラメーターで、直接の関係がない。しかし、s.wsum · mは、1/gを正規化するために作用する必要があるが、これでは正規化できないはずだ。\nもし、全てのサンプリング戦略でMが1であると仮定すると、g(r.y) · r.wの部分はg/pに相当して、s.wsumは、サンプリング戦略数分の$\\sum{g/p}$で、サンプリング戦略数で割ればg/pの平均値として解釈できる。 一方でmは、各サンプリング戦略のターゲット分布関数が同じものだった場合は、サンプリング戦略数の逆数となる。選択されたサンプルのターゲット分布関数が、 他の戦略に対して相対的に大きければ、ウエイトが大きくなり、逆ならば小さくなるようなMISウエイトとして解釈できる。\nfunction combineReservoirsMIS(g, r1, r2, . . . , rk, g1, g2, . . . , gk) Reservoir s // a new reservor foreach r ∈ {r1, . . . , rk } do s.update(r.y, g(r.y) · r.w · r.M, r.M) // calculate normalized MIS weight using g() g_sum ← 0 foreach g_j ∈ {g1, . . . , gk } do g_sum ← g_sum + g_j(r.y) m ← g_*(r.y) / g_sum // g_* : target distribution that contributed s.y w ← (1 / g(s.y))(s.wsum · m) return s.y, w  私の考えでは、上記の記載は、すべてのRISのMが1である場合は正しく動作するが、それ以外では誤りで、選択されたRISのMの値$M_s$で除算しなくてはならないはずである。\n w ← (1 / (g(s.y) * Ms)) (s.wsum · m)  ","date":1599793175,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599793175,"objectID":"9b6cdab9a0fae2185d4774220e711a0e","permalink":"https://shikihuiku.github.io/memo/restir/","publishdate":"2020-09-11T11:59:35+09:00","relpermalink":"/memo/restir/","section":"memo","summary":"ReSTIRのためのメモ書き","tags":[],"title":"Importance Resampling 関連","type":"memo"},{"authors":[],"categories":[],"content":"これは補完資料です この記事は、CEDEC2020での講演 \u0026ldquo;Direct3D 12 Device Removalの処方箋\u0026rdquo; において、時間内に説明することができなかった部分に関して解説するためのものです。 CEDEC2020で当該講演を聴講された方に向けて書いています。この記事単体では不完全です。タイムシフト視聴や、CEDiLにアクセス可能な方は、先にそちらをご覧になることをお勧めします。\nDEVICE_REMOVEDとは   DXGIとD3D12API返すHRESULTに設定されるエラー\n 正式にはDXGIのエラーコード。DXGI_ERROR_DEVICE_REMOVED 殆どの場合は、IDXGISwapChain::Present()呼び出しの際に返される ID3D12DeviceD3D12の一部のメソッド、リソースの作成、Mapなどを実行した際にも返される ID3D12Device::GetDeviceRemovedReasonの呼び出しでも返される    ID3D12Device::GetDeviceRemovedReasonを呼び出すことで以下の様な具体的なエラー原因が取得できる。\n DXGI_ERROR_DEVICE_HUNG DXGI_ERROR_DEVICE_REMOVED DXGI_ERROR_DEVICE_RESET DXGI_ERROR_DRIVER_INTERNAL_ERROR DXGI_ERROR_INVALID_CALL    FormatMessage()や、_com_errorでエラーの意味を取得できる\nDevice Removed Reason for 887a0006 DXGI_ERROR_DEVICE_HUNG The GPU will not respond to more commands, most likely because of an invalid command passed by the calling application.\n  DEVICE_REMOVEDが発生する原因について DEVICE_REMOVEDは、D3D12APIを通じて、GPUやドライバーで発生したエラーの結果に過ぎない。OSやD3D12ランタイムが、コンテキストの実行を継続するべきでは無いと判断した場合に発生する。 ただ、 Alex DunnがGDC2018で説明した通り、大きく分けて２つの種類にカテゴライズする事ができる。\n  TDR（Timeout Detection and Recovery）によるDEVICE_REMOVED\nドライバーやGPUがOSに対して一定時間内に応答しなかった場合に、OSが発生させるDEVICE_REMOVED。OSはシステム全体のHungを避けるため、DEVICE_REMOVEDを発生させてドライバーをリセットする。\n ドライバーのコードパスで想定していない長時間の処理があった場合 シェーダー内で長時間処理がかかった場合（シェーダー内無限ループ等） Signal,Waitの設定ミスで長時間Fenceが解決しなかった場合    エラーの検出によるDEVICE_REMOVED\n何らかの看過できないエラーの発生に伴いOSやD3D12ランタイムが発生させるDEVICE_REMOVED。\n GPUで発生したPage Fault 存在しないリソースへのアクセスや、宣言した利用用途と異なるアクセス。 不正な上書き等によるCommand Listの破損 結果的にドライバーやGPUが不正な実行コマンドを受け取る。 D3D12ランタイムやドライバーによるエラーの検出 許可されていないリソースステートのリソースへのアクセス。各種リソースのアラインメント違反。    GPUとCPUの時間のずれ ここでは、CPUコードのデバッグと、DEVICE_REMOVEDの追跡の決定的な違いについて説明する。 CPUの実行コードは、デバッガがアタッチされている状況下では即時的であり、エラーが発生すれば直ちにプログラムの実行を停止して、デバッガに処理を返すことで、エラーが起きた瞬間の状況が分かる。\nこれに対して、DEVICE_REMOVEDの発生は、CPUのコードと全く同期しないタイミングで発生する。そのため、CPUがDEVICE_REMOVEDを受け取った瞬間にデバッガで処理を止める事にはほとんど意味がない。\n以下のスクリーンショットはGPUViewというツールでCPUとGPUの処理時間を示したものになる。画面左から右に時間の経過を表している。中央の大きなスタックの中でハイライトされているのは、 あるGPU処理の塊となる、バケットである。ご覧の通り画面の左端で生成されたバケットは、画面の右側でスタックの最下段に到達している。この時点GPUの処理の対象となる。この間3フレーム分の時間が経過している。 もし、このGPU処理のなかでDEVICE_REMOVEDが発生したら、CPUがそのエラーを受け取る可能性があるのは、この時点以後となるので、CPUから見るとコマンド生成から3フレーム以上遅れてDEVICE_REMOVEDを受け取る事になる。\n  GPUとCPUの処理時間のずれ   これが、DEVICE_REMOVEDの追跡が難しい原因の一つである。\nDEVICE_REMOVEDの対処法 GPU上で発生する様々なエラーをデバッグする方法として、D3D12APIは以下の方法を提供している\n Debug Layer\n昔からあるが、DEVICE_REMOVEDの原因の追跡において最も有効な方法の一つ GPU Based Validation\n比較的新しく導入されたDebug Layerの拡張。CPU側のValidationでは追跡できない問題を検出する DRED1.2\n新しく導入されたDEVICE_REMOVEDの追跡方法  上記3つのうち、先の二つは、DEVICE_REMOVEDが発生する前に起きているD3D12上のエラーの追跡に使うのに対して、 DREDは、DEVICE_REMOVEが発生した後に、発生した箇所を見つけ出すためのもので、用途が完全に異なる。どちらも有用なので組み合わせて使う。\nDebug Layer DEVICE_REMOVEDに対する処方の第一候補は、Debug Layerである。これを有効にすることにより、D3DのランタイムがValidationを積極的に行い、Debug Outputにメッセージを送出するようになる。 DEVICE_REMOVEDが発生する前に出力されるDebug Layerのメッセージは、DEVICE_REMOVEDの発生原因を調査する上での貴重な手がかりになる。\nDebug Layerの有効化 Debug Layerはアプリケーション自身で有効にすることもできるし、外部から強制的に有効にすることもできる。\n外部から強制的に有効にする際は、dxcpl.exe(GUIツール)やd3dconfig.exe(コマンドラインツール)を用いる。インストールはWindows10の、Settings→Add an optional feature→ Add a feature→ Graphics Toolsを選択する事で行う。\n  dxcplのインストール   外部からDebug Layerを有効にする際は、dxcpl.exeかd3dconfg.exeを用いて、ターゲットとなるアプリケーションの名前を事前に登録し、Debug Layerを強制的に有効にする設定にする。設定内容はdxcplとd3dconfigで共有され、システム全体で有効になるので注意が必要である。\n  デバッグ対象アプリケーションを登録する     デバッグ対象アプリケーションを登録する   アプリケーション内部で設定する場合は、CreateDeviceを実行する前に、ID3D12Debugインターフェースを取得して、EnableDebugLayer()を呼び出す事で有効にできる。 この場合は、dxcpl.exeやd3dconfig.exeによるターゲットアプリケーション名の登録は必要ない。登録してある場合は、debug-layerの設定はApplication Controlledに設定することでAPIから明示的に有効にした場合のみDebug Layerが有効になる。\n// Create Deviceの前に設定する { ComPtr\u0026lt;ID3D12Debug1\u0026gt; debug1; if (SUCCEEDED(D3D12GetDebugInterface(IID_PPV_ARGS(\u0026amp;debug1)))) { debug1-\u0026gt;EnableDebugLayer(); } }  Debug Layerの出力 Debug Layerが有効になっている状態では、アプリケーションのD3DAPIの使用において何らかの間違いが検出されれば、エラーの内容がデバッグ出力ストリームに文字列として出力される。出力メッセージはVisual StudioやDbgviewなどのツールを使って確認することができる。出力内容は、その深刻度に応じてグループ分けされている。\n Info\nリソースの確保や開放などを通知する。デフォルトでMuteされている。 Warning\nAPIの仕様から逸脱していないが、パフォーマンスの問題や、バグの発生の原因になりそうな状況を通知する。 Error\nAPIの仕様から逸脱した状況が検出された場合に通知する。ただ、これが出力されるから、直ちにDEVICE＿REMOVALが発生するという訳ではない。 Corruption\nリソースやオブジェクト（オブジェクト自身というよりは、多くはそのハンドル等）が破損していることが検出された場合に通知する。 Message\n上記に当てはまらない情報を通知する（メモリ不足等）  以下は、例としてResourceBarrierの遷移前リソースステートの指定が間違っていた場合に出力されたエラーである。ちなみにこのプログラムは、Debug Layerが無効な状態でも有効な状態でも正常に動作した。\nD3D12 ERROR: ID3D12CommandList::ResourceBarrier: Before state (0x0: D3D12_RESOURCE_STATE_[COMMON|PRESENT]) of resource (0x000001AE3B886890:'MyColorTex') (subresource: 0) specified by transition barrier does not match with the current resource state (0x400: D3D12_RESOURCE_STATE_COPY_DEST) (assumed at first use) [ RESOURCE_MANIPULATION ERROR #527: RESOURCE_BARRIER_BEFORE_AFTER_MISMATCH] D3D12 ERROR: ID3D12CommandQueue::ExecuteCommandLists: Using ResourceBarrier on Command List (0x000001AE3B802060:'MyCommandList_Direct'): Before state (0x0: D3D12_RESOURCE_STATE_[COMMON|PRESENT]) of resource (0x000001AE3B886890:'MyColorTex') (subresource: 0) specified by transition barrier does not match with the state (0x400: D3D12_RESOURCE_STATE_COPY_DEST) specified in the previous call to ResourceBarrier [ RESOURCE_MANIPULATION ERROR #527: RESOURCE_BARRIER_BEFORE_AFTER_MISMATCH]  Debug Layerはこのエラーを二か所で検出した。一つはID3D12CommandList::ResourceBarrier()呼び出し時に、もう一つは、ID3D12CommandQueue::ExecuteCommandLists()呼び出し時に検出した。しかしこれは、この種のエラーは常に二か所で検出されるという意味ではない。コマンドリストは他のコマンドリストの生成タイミングと関係なく生成する事ができ、その際のコマンドリスト作成時のリソースのステートは未確定になる場合がある。そのためDebug Layerは複数の箇所で可能な限りエラーの特定を試みる。上記の場合では、コマンドリスト作成時の対象リソースの事前ステートが確定できたので、ID3D12CommandList::ResourceBarrier()の呼び出し時にエラーが出力出来たという事である。\nまた、ステートが間違っていたリソースの名前が、\u0026lsquo;MyColorTex\u0026rsquo;といった様に表示されるが、これはアプリケーション自身が、ID3D12Object::SetName()を通じて設定したものである。D3D12アプリケーションを開発し、各種デバッグ機能を使う予定がある場合は、可能な限り全てのD3D12Objectに名前をつけるべきである。すると、上記の様にエラーが発生した際のメッセージによって原因となったリソースの特定が簡単に行えるようになる。Command ListやDescriptor Heapなどにもしっかりと名前を付けると、上記の様にエラーが発生したコマンドリスト名からエラーがどのレンダリングパスで発生したのかが特定できる場合もある。また、PIXやNSightといったフレームプロファイラを使う場合にもこれらの名前付けは有用である。\n次の例は、RenderTargetを設定したクリアカラー以外でクリア場合に発生する警告である。これはエラーではないので無視しても構わない。しかし、このようにパフォーマンスの向上を考える場合に有用なメッセージが得られる場合もある。\nD3D12 WARNING: ID3D12CommandList::ClearRenderTargetView: The application did not pass any clear value to resource creation. The clear operation is typically slower as a result; but will still clear to the desired value. [ EXECUTION WARNING #820: CLEARRENDERTARGETVIEW_MISMATCHINGCLEARVALUE]  ID3D12InfoQueueについて Debug Layerは、時にはアプリケーションが意図して記述しているコードに対してもメッセージを出力する場合がある。その場合は、アプリケーションが無視するべきと考えるメッセージを、D3D12InfoQueueを使ってフィルタリングできる。以下のコードスニペットは、GPUが書き込みしている可能性のあるリソースがCPUから読み込み可能な状態でMapされている場合に出力される警告を抑制するためのものである。\nComPtr\u0026lt;ID3D12InfoQueue\u0026gt; d3dInfoQueue; if (SUCCEEDED(device-\u0026gt;QueryInterface(IID_PPV_ARGS(\u0026amp;d3dInfoQueue)))) { // Suppress individual messages by their ID. D3D12_MESSAGE_ID denyIds[] = { D3D12_MESSAGE_ID_EXECUTECOMMANDLISTS_GPU_WRITTEN_READBACK_RESOURCE_MAPPED, }; D3D12_INFO_QUEUE_FILTER filter = {}; filter.DenyList.NumIDs = _countof(denyIds); filter.DenyList.pIDList = denyIds; d3dInfoQueue-\u0026gt;AddStorageFilterEntries(\u0026amp;filter); OutputDebugString(L\u0026quot;Warning: GPUTimer is disabling an unwanted D3D12 debug layer warning: D3D12_MESSAGE_ID_EXECUTECOMMANDLISTS_GPU_WRITTEN_READBACK_RESOURCE_MAPPED.\u0026quot;); }   Microsoft DirectX SDK Sampleより引用\n メッセージのフィルタリングは、InfoQueueを通じてではなく、dxcpl/d3dconfigを使っても同様のフィルタリングの設定が可能だが、メッセージのフィルタリングはアプリケーションごとに行われるべきであるので、通常はアプリケーションのコードに記述されるべきである。ちなみに、InfoQueueの設定は、dxcpl/d3dconfigの設定でオーバーライドされるので、InfoQueueを使って制御したいときは、dxcpl/d3dconfigにアプリケーションを登録してはいけない。 以下はID3D12InfoQueueのその他の機能についてである。\n  InfoQueueのデフォルト設定では、Infoレベルのメッセージはフィルタリングされているので、Infoレベルのメッセージを取得する必要がある場合はフィルタの設定を一旦クリアする必要がある。\n  フィルターにはStorageFilterとRetrievalFilterの二種類がある。\nStorageFitlerは、エラーがメッセージキューにストアするときに適用されるフィルタ。フィルターを通過できなければ、メッセージキューにストアされない。 RetrievalFilterはメッセージを取得する際に適用されるフィルタ。メッセージキューにストアされているメッセージを破壊せずに、特定の種類のメッセージのみを抽出したいときなどに使う。\n  SetMuteDebugOutputでデバッグ出力ストリームへの出力を停止できる。 アプリケーション側で出力されるエラーのハンドリングを全て行う場合などで、デバッグ出力ストリームへの出力が不要な場合は抑止できる。\n  特定のエラーが検出された時や、エラーの深刻度によって、DebugBreakすることが可能。 Debug LayerはCPU側のD3D12ランタイムがエラーを検出しているので、エラーが発生するタイミングは、CPU処理と同期したタイミングが多い。したがって、DebugBreakすることは有効である。 しかし、DebugBreakがかかるのは、D3Dのランタイム側のスレッドでかかる場合もあるので、追跡するには、マルチスレッドのデバッギングが必要になる。\n  GPU Based Validationの有効化 DEVICE_REMOVEDへの処方の第二候補は、GPU Based Validationの有効化である。GPU Based Validation(以下GBV)は、その名の通り、GPU側での実行時に行うValidationである。 GBVもアプリケーション自身で有効にすることもできるし、dxcplなどで強制的に有効にすることもできる。この点はDebug Layerと同様である。なお、Debug Layerが有効化されていないと動作しないので、Debug Layerの拡張機能と考える事もできる。\n{ ComPtr\u0026lt;ID3D12Debug1\u0026gt; debug1; if (SUCCEEDED(D3D12GetDebugInterface(IID_PPV_ARGS(\u0026amp;debug1)))) { debug1-\u0026gt;EnableDebugLayer(); debug1-\u0026gt;SetEnableGPUBasedValidation(true); } }  先ほど説明したDebug Layerは主にCommandListに命令を積み、ExecuteCommandListを呼び出すまでに行われるValidation。対してGBVはシェーダー実行時に行われるValidationになる。 未定義のDescriptorや、廃棄済みのリソースへのアクセス。不適切なリソースステートでのアクセスなど、CommandList作成時には、リソースの状況が未定で、検出できないエラーを実行時に検出する。 メッセージは既存のDebug Layerと同様に出力されるが、その出力のタイミングはコマンドリストを生成したCPU処理と同期しない。したがって、エラーメッセージが出力された瞬間のCPU処理を検証しても意味がない。\n D3D12_MESSAGE_ID enumerationを確認すれば、GBVで出力されるメッセージのIDには、 \u0026ldquo;GPU_BASED_VALIDATION\u0026quot;が含まれるのが分かる。これで実際にどのようなエラーが検出可能なのか分かる。\nGBVは、シェーダーコードとPSOにパッチを充てる形で実現する。これらには、いくつかのモードがあり選択することができる。GBVの設定は以下のAPIと構造体を通じて設定を行う。\nID3D12DebugCommandList1::SetDebugParameter() typedef struct D3D12_DEBUG_DEVICE_GPU_BASED_VALIDATION_SETTINGS { UINT MaxMessagesPerCommandList; D3D12_GPU_BASED_VALIDATION_SHADER_PATCH_MODE DefaultShaderPatchMode; D3D12_GPU_BASED_VALIDATION_PIPELINE_STATE_CREATE_FLAGS PipelineStateCreateFlags; } D3D12_DEBUG_DEVICE_GPU_BASED_VALIDATION_SETTINGS;  以下はシェーダーのパッチモードの選択である\n  NONE\nシェーダーコードにValidationコードを挿入しないモード。 CommonStatePromotionによるリソースステートの遷移をトラッキングすることができない。そればかりかGBVを混乱させる恐れがある。\n  TRACKING_ONLY_SHADERS\nリソースステートの遷移のみをチェックするためのコードが挿入される。\n  CREATE_UNGUARDED_VALIDATION_SHADERS\nGBVのValidationコードが挿入される。Validationによるエラーが検出され、無効なリソースに対するアクセスや範囲外アクセスがあっても該当コードを実行する。結果、DEVICE_REMOVEDなどを引き起こすかもしれない。これがデフォルトのシェーダーパッチモード。\n  CREATE_GUARDED_VALIDATION_SHADERS\nGBVのValidationコードが挿入される。Validationによるエラーが検出された場合は、該当のリソースアクセスを避ける。\n  PipelineStateCreateFlagsでは、事前にPatchされたPSOを生成するかどうかを制御できる。 デフォルトでは、パッチがあてられたPSOの初回使用時にコンパイルされる挙動なので、CommandListのRecordingが遅くなる。FRONT_LOADを設定することで予めコンパイルされる設定になる。\n以下はGBVによって検出されたエラーの一例。UAVの範囲外にシェーダーがアクセスしたことで出力された。この種のバグは、CPU側のDebug Layerでは検出できないが、GBVならば検出できる。\nDescriptorTableのUAVに設定したUAVバッファに対する範囲外アクセス　(RootSignature1.1を使用。Range Flagは　D3D12_DESCRIPTOR_RANGE_FLAG_DATA_STATIC_WHILE_SET_AT_EXECUTE) D3D12 ERROR: GPU-BASED VALIDATION: Draw, Resource access out of bounds: Resource: 0x000001C6F8F91A60:'DummyResource_256_bytes_UAV_buffer', Descriptor Type: UAV, Highest byte offset from view start accessed: [439737], Bytes available in view: 256. Results undefined because descriptor is declared static in root signature, which allows hardware/driver the option of converting the access to a root descriptor. Unlike descriptor heap descriptors, root descriptors do not have defined behavior for an out of bounds access. Index of Descriptor Range: 1, Shader Stage: PIXEL, Root Parameter Index: [0], Draw Index: [0], Shader Code: \u0026lt;debug info not available\u0026gt;, Asm Instruction Range: [0xbc-0xdf], Asm Operand Index: [2], Command List: 0x000001C6F8E6DA10:'MyCommandList_Direct', SRV/UAV/CBV Descriptor Heap: 0x000001C6F8D8AB60:'Unnamed ID3D12DescriptorHeap Object', Sampler Descriptor Heap: \u0026lt;not set\u0026gt;, Pipeline State: 0x000001C6F8BC81B0:'Unnamed ID3D12PipelineState Object', [ EXECUTION ERROR #1005: GPU_BASED_VALIDATION_RESOURCE_ACCESS_OUT_OF_BOUNDS]  ここで、GBVの話から少しそれるが、このエラーについて詳しく考えてみたいと思う。また、これらの出来事は私のローカル環境で観測されたに過ぎないことも明記しておく。 上記のエラーメッセージを要約すると以下の通りと思われる。\nリソースへの範囲外アクセス。リソース：`ummyResource_256_bytes_UAV_buffer` デスクリプタタイプ:UAV 最高でオフセット[439737]にアクセスした。Viewでアクセス可能なのは 256. アクセスの結果は未定義です。なぜなら、デスクリプタはRootSignatureで`static`として宣言されており、ハードウェアやドライバーはこの（メモリ）アクセスをルートデスクリプタにコンバートする選択肢が許されているからです。 デスクリプタヒープのデスクリプタと異なり、ルートでスクリプタには範囲外アクセスの挙動の定義がありません。  このUAVはDescriptorTableに定義したが、RangeFlagに、D3D12_DESCRIPTOR_RANGE_FLAG_DATA_STATIC_WHILE_SET_AT_EXECUTEを設定した。このフラグが設定されたものはドライバーの最適化対象になる可能性があり、RootDescriptor（RootTableに直接定義するDescriptor）にコンバートされる可能性がある。 実際にコンバートされた場合は、範囲外アクセスは未定義動作となるので、エラーになっているという訳である。しかし、実際はリソースのアクセス範囲チェックがされていた（つまり、RootDescriptorへのコンバートは行われていなかった）ので、DEVICE_REMOVEDが発生するような致命的な事態にはならなかった。\n次に、このUAVが設定されているDescriptorTableのRangeFlagに、D3D12_DESCRIPTOR_RANGE_FLAG_DESCRIPTORS_VOLATILEを設定するとどうなるかというと、エラーが全く出力されなくなった。これは、DirectXの仕様として、RootSignature1.1のDescriptorTableに定義されたUAVで、D3D12_DESCRIPTOR_RANGE_FLAG_DESCRIPTORS_VOLATILEを設定された場合、もしくはRootSignature1.0で定義されたUAVの場合は、リソースアクセスの範囲チェックが行われる決まりがある。範囲外の読み出しはゼロを返され、範囲外への書き込みは行われない。DirectXの仕様に則った動作なのでエラーが発生しないというわけである。\n次は、DescriptorTableを介さずに、直接RootTableにUAVを定義して、範囲外アクセスを起こすと以下のメッセージが出力された。\nRootTableに設定したUAVバッファに対する範囲外アクセス D3D12 ERROR: GPU-BASED VALIDATION: Draw, Root descriptor access out of bounds (results undefined): Resource: 0x000001A7600AF410:'DummyResource_256_bytes_UAV_buffer', Root Descriptor Type: UAV, Highest byte offset from view start accessed: [803581], Bytes available from view start based on remaining resource size: 256. Shader Stage: PIXEL, Root Parameter Index: [1], Draw Index: [0], Shader Code: \u0026lt;debug info not available\u0026gt;, Asm Instruction Range: [0xc8-0xeb], Asm Operand Index: [2], Command List: 0x000001A75F82C5B0:'MyCommandList_Direct', SRV/UAV/CBV Descriptor Heap: 0x000001A75F9DEA70:'Unnamed ID3D12DescriptorHeap Object', Sampler Descriptor Heap: \u0026lt;not set\u0026gt;, Pipeline State: 0x000001A75FDC5DE0:'Unnamed ID3D12PipelineState Object', [ EXECUTION ERROR #961: GPU_BASED_VALIDATION_ROOT_DESCRIPTOR_ACCESS_OUT_OF_BOUNDS] さらに、DEVICE_REMOVED発生した。 D3D12: Removing Device. D3D12 ERROR: ID3D12Device::RemoveDevice: Device removal has been triggered for the following reason (DXGI_ERROR_DEVICE_HUNG: The Device took an unreasonable amount of time to execute its commands, or the hardware crashed/hung. As a result, the TDR (Timeout Detection and Recovery) mechanism has been triggered. The current Device Context was executing commands when the hang occurred. The application may want to respawn and fallback to less aggressive use of the display hardware). [ EXECUTION ERROR #232: DEVICE_REMOVAL_PROCESS_AT_FAULT]  先ほどとエラーメッセージが異なり、エラーのIDが異なるので注意が必要である。以上の出来事をまとめると以下の様になる。\n DescriptorTableに定義した場合\n#1005: GPU_BASED_VALIDATION_RESOURCE_ACCESS_OUT_OF_BOUNDS\nこちらのエラーは、VOLATILEでないDescriptorTableに定義されたリソースに対する範囲外アクセスで発生したエラー。 ハードウェアやドライバーが、範囲外アクセスを未定義動作にすることが許されている状態だが、実際に範囲外アクセスをするかは実装次第。 RootTableに直接定義した場合\n#961: GPU_BASED_VALIDATION_ROOT_DESCRIPTOR_ACCESS_OUT_OF_BOUNDS\nこちらは、DescriptorTableではなく、RootTableに定義されたリソースの範囲外アクセスで発生したエラー。 RootTableにUAVやSRVを定義した場合、リソースのサイズは格納されない事が知られており、通常は範囲外アクセスへのチェックも行われない事が知られている。しかし、GBVを有効にすることでこれらの範囲外アクセスがValidatorにより検出され、 エラーが出力されたという状態。  このように、エラーメッセージから学べる事もあるので、Debug LayerやGBVを有効にするのはおすすめである。\nDebug Layerのその他の機能 Synchronized Command Queue Validation Debug Layerを有効にすることで、Synchronized Command Queue Validationという機能がでデフォルトで有効になる。 この機能によって、FenceのWaitが設定されたコマンドリストにおいて、Waitの条件が満たされるまで、GPUへのコマンド送出をしなくなる。 これにより、Waitが設定されている以降のコマンドにおけるリソースステートをCPU側でも確認することができ、結果として、コマンド送出時にリソースステートのValidationをより厳密に行う事ができる。 Disableにすることによって、FenceのSignalとWaitを多用したQueueの組み立てをしている場合に限り、Debug Layer使用時の若干のパフォーマンス向上が期待できるが、そもそもDebug Layerはパフォーマンスを追求するためのものでは無いのでDisableにするメリットは殆どない。\nDebugDevice / DebugCommandQueue / DebugCommandList Debug Layerが有効な状態では、Device, CommandQueue, CommandListからQueryInterfaceすることで、表題のインターフェースが取得できる。 主な機能は以下の通り。\n ID3D12DebugDevice::ReportLiveDeviceObjects()\n現在有効なオブジェクトをデバッグ出力ストリームに出力する。 ID3D12DebugCommandList::AssertResourceState()\nリソースのステートが、呼び出し引数に与えたステートと等しいかを返す。\nCommon State Promotionを使う場合は、これでState PromotionやDecayの確認をするとデバッグしやすい。 ID3D12DebugCommandQueue::AssertResourceState()\nリソースのステートが、呼び出し引数に与えたステートと等しいかを返す。\nCommandQueuから直接リソースを操作するAPIがある関係上、CommandQueuからもリソースのステートが確認できる。  Device Removed Extended Data 1.2  Device Removed Extended Dataとは、実際にDEVICE_REMOVEDが発生した後に、発生のより詳しい状況を知るための機構である。通常はDEVICE_REMOVEDが発生しても、得られる情報はせいぜいHRESULTのエラーコードぐらいで、デバッグの指標となる情報はほとんどない。しかし、DREDを活用すれば、DEVICE_REMOVEDが発生した時にGPUが実行していたコマンドや、原因となったメモリアクセスについて知ることができる場合がある。 Debug Layerとは機能的に独立しているので、使用にあたりDebug Layerを有効にする必要はない。また、Debug Layerほど処理オーバーヘッドが大きくないので、常時有効にしてアプリケーションを開発することができる。 以下は、DREDの主要機能を有効にするためのコードスニペットである。DRED自体はWindowsSDKの10.0.18362.1より使用可能だが、一部重要な機能が未実装なので、WindowsSDKの10.0.19041.0以後の導入とWindows10 20H1の導入を推奨する。\n// Try enabling DRED even in release code { ComPtr\u0026lt;ID3D12DeviceRemovedExtendedDataSettings1\u0026gt; d3dDredSettings1; if (SUCCEEDED(D3D12GetDebugInterface(IID_PPV_ARGS(\u0026amp;d3dDredSettings1)))) { // Turn on AutoBreadcrumbs and Page Fault reporting d3dDredSettings1-\u0026gt;SetAutoBreadcrumbsEnablement(D3D12_DRED_ENABLEMENT_FORCED_ON); d3dDredSettings1-\u0026gt;SetBreadcrumbContextEnablement(D3D12_DRED_ENABLEMENT_FORCED_ON); d3dDredSettings1-\u0026gt;SetPageFaultEnablement(D3D12_DRED_ENABLEMENT_FORCED_ON); } }  Auto BreadcrumbsとBreadcrumb Contextについて Breadcrumbsは、パンくずのことで、所謂通ってきた道を見失わないためにパンくずを撒きながら森の中を歩いた童話にちなんでいる。Auto Breadcrumbsは、明示的にAPIを呼び出してパンを撒かなくても自動的に道標なるイベント（D3D12のAPI呼び出し）を自動的に記録するための機能である。 Auto Bredcrumbsが記録するのは、基本的には、CommandListを介して実行するコマンド群である。詳細は D3D12_AUTO_BREADCRUMB_OP enumerationで確認できる。 そして、DEVICE_REMOVEDが発生する直前に実行したメソッドを指し示すことで、DEVICE_REMOVEDが発生した瞬間にGPUが実行していたオペレーションが分かる仕組みになっている。\nしかし、Auto Bredcrumbsは実行したコマンドの種類を記録するだけなので、連続する一連のDrawなどでは、実際にどのDraｗコールが問題を引き起こしたか分からない。 Breadcrumb Contextは、Auto Breadcrumbによって記録されたオペレーションに関連する情報を記録した文字列が取得できるDRED1.2で導入された新しい機能である。 具体的には、Pixのマーカーがセットされた場合は、そのマーカーの文字列が記録される。これにより、大幅にレンダリング箇所の特定が行いやすくなった。\nGPU Page Faultについて GPU Page Faultは、GPU上で発生する不正なメモリアクセスで、これが発生するとDEVICE_REMOVEDとなる。DREDはGPU Page Faultの情報を記録する。まずはGPU Page Faultを理解するためにGPU仮想アドレス空間について簡単に説明する。\nGPU仮想アドレス空間について  GpuMmuは、WDDM2.0(Windows Display Driver Model 2.0)でサポートされている、主にディスクリートGPU（VRAMとシステムメモリが物理的に独立しているGPU）のための 仮想アドレスモデルである。このモデルでは、プロセスごとに、GPU仮想アドレス空間がCPUの仮想アドレス空間とは別に存在して、物理アドレスに変換するためのMMUも、CPUのMMUとは別に存在している。 GPU仮想アドレス空間は、その名の通りGPU上で実行されるシェーダー等からメモリアクセスをする際に使用されるアドレス空間である。CPU側(D3D12APIやドライバー)でのリソース確保や解放によって、物理メモリが確保または破棄されて、アドレス変換テーブルが更新される。 アドレス変換テーブルが更新される際にはGPU側と同期して、GPU側と同じアドレス変換情報を共有することで、GPU上での仮想アドレスにおけるメモリアクセスを実現している。 図にある通り、物理リソースへのアクセスはアドレステーブルによる変換を介して行う。また、マップされるメモリは、VRAMでもSysMemでも構わない。GPUはどちらに配置されているリソースでも、透過的にアクセスすることができる。\nGPU Page Faultが起きるケース GPUがPage Faultを起こすのは、アクセスが許されないページにアクセスした場合や、そもそもメモリがマッピングされていないアドレスにアクセスした場合である。主に具体的なケースとして考えられるのは、以下の通りである。\n DrawcallやDispatch,Copy処理などにおいて、すでに破棄したリソースを参照した場合。 DrawcallやDispatch,Copy処理などにおいて、Evictしたリソースや、Non-Regidentなタイルリソースを参照した場合。 DrawcallやDispatchで、未初期状態のDescriptorTableや、誤ったDescriptorTableを参照した場合。 DrawcallやDispatchで、可変長のDescriptorTableで、シェーダーが実際に配置されているテーブルの範囲を逸脱してアクセスした場合。 DrawcallやDispatchでRootTableに配置したUAVやSRVに対して誤った範囲でアクセスした場合。  GPU Page Faultで得られる情報について DREDは、PageFaultが発生したアドレス空間に確保されているオブジェクトが有れば、そのオブジェクト名（SetNameで付けた名前）が記録される。 またAllocationTypeとして、そのアドレス空間に配置されたオブジェクトが、 どのような種類であるかを知ることができる。 また、そのアドレス空間を使っていて、直近で解放されたリソースがあれば、そのリソースの情報が取得できる。これは、解放されたリソースに対して、シェーダー等がアクセスした場合に発生するPage Faultを知るのに特に有用である。 しかし、GPU Page Faultはあくまで、GPU仮想アドレス変換時のエラーでしかないので、アクセスしたアドレスに有効なページがあればアクセス自体が成立するため、GPU page faultにならない。したがって、すべての不正アクセスを検出するわけではない。 たとえば、EvictしたリソースはVRAMが特に逼迫した状況になるまではリソースのページアウトが起きないため、そのままVRAM上に配置されていることが多い。結果Page Faultも起きない上に、正しくレンダリングされるため、問題に気づけない。\nDREDで得られる情報で何が分かるか DREDは、一見するとDEVICE_REMOVEDの発生原因についての十分な情報を提供してくれるように思えるが実際は違う。 AutoBreadCrumbは、エラーが発生していた時に実行していたコンテキストに過ぎず、実際にエラーの原因がその中にあるとは限らない。 Page Faultも同様で、Page Faultは発生した一つのアクセス例外に過ぎず、何がアクセス例外の原因となったかは分からない。たとえば、それが古いDescriptor Tableを参照したことによるのか、 破損したDescriptor Tableを参照したことによるのか、参照しているリソースを開放してしまったことによるのかは分からない。\nしかし、DEVICE_REMOVEDが頻発する状況下では、DREDで複数のクラッシュの情報を集約することは非常に有効である。例えば、もしも、PageFaultがいつも同じリソースとアドレスで発生するとしたら、 プログラムのロジックが安定的な間違いを犯している可能性が高いと考えられる。また、そうではなく、PageFaultがいろいろなリソースやアドレスで発生するとしたら、リソースやDescritorTableを管理しているスレッドと GPUの実行コンテキストのレースコンディションを調べる価値があると考えられる。AutoBreadCrumbも同様で、毎回同じドローコールでDEVICE_REMOVEDが発生しているならば、 該当ドローコールのロジックや、実行分岐制御に関わる変数やリソースを調べるべきだが、異なるドローコールでランダムにDEVICE_REMOVEDが発生するならば、コマンドリストの破損の可能性が考えられる。\n以下はCommandList作成時には存在していたTextureがExecuteCommandListsの前に解放された場合に発生するGPU Page Faultによって発生した、DEVICE_REMOVEDの際に取得できたDREDの情報である。なお、DREDの情報はデバッグ出力ストリームに自動的に出力されないので、 自身でデータにアクセスして、何らかの形で表示する必要がある。\nDXGI_ERROR_DEVICE_HUNG The GPU will not respond to more commands, most likely because of an invalid command passed by the calling application. ==== Auto Breadcrubs ==== QueueNameW: MyCommandQueue QueuePtr: 0x2bad9c40330 BreadcrumbCount: 0 BreadcrumbContextsCount: 0 LastBreadcrumbValues: 0 ==== Auto Breadcrubs ==== QueueNameW: MyCommandQueue QueuePtr: 0x2bad9c40330 CommandListNameW: MyCommandList_Direct CommandListPtr: 0x2bad9e379f0 BreadcrumbCount: 7 BreadcrumbContextsCount: 3 LastBreadcrumbValues: 5 0|D3D12_AUTO_BREADCRUMB_OP_SETMARKER|==Frame Start== 1|D3D12_AUTO_BREADCRUMB_OP_SETMARKER|Set viewport and render targets 2|D3D12_AUTO_BREADCRUMB_OP_RESOURCEBARRIER 3|D3D12_AUTO_BREADCRUMB_OP_CLEARRENDERTARGETVIEW 4|D3D12_AUTO_BREADCRUMB_OP_SETMARKER|Draw - Triangle \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;Something wrong happned here...\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 5|D3D12_AUTO_BREADCRUMB_OP_DRAWINSTANCED 6|D3D12_AUTO_BREADCRUMB_OP_RESOURCEBARRIER ====Page fault information ==== PageFaultGPUVA: 0x70fc000 ==Existing Allocation Node Info ==Recent Freeed Allocation Node Info ObjectNameW: DummyResource_256_bytes_UAV_buffer AllocationType: D3D12_DRED_ALLOCATION_TYPE_RESOURCE IUnknownPtr: 0x0x2bad9e8f9c0 D3D12app.exe has triggered a breakpoint.  Dump File について DREDの情報はユーザーモードダンプからも抽出することができる。まずは、 プロセスがCrashした際に、FullDumpが作られる様に事前に設定し、ダンプファイルをwindbgで読み込む。 windbg.exeはWindows10のSDKに同梱されている。通常は、\u0026ldquo;C:\\Program Files (x86)\\Windows Kits\\10\\Debuggers\\x64\\windbg.exe\u0026quot;に配置されるはずである。 そこで、 MicrosoftがGitHubで公開しているスクリプトを読み込むことで、DREDの情報に容易にアクセスできる。 手順は該当のリポジトリでも確認できるが非常に簡単である。プロセスがクラッシュした際のフルダンプを読み込み、以下のコマンドを実行するだけである。\n.scriptload \u0026lt;\u0026lt;path to script file\u0026gt;\u0026gt;\\d3ddred.js !d3ddred  以下が、Windbg上で実際にDRED情報を表示した例である。取得できる情報は、DREDのAPIで取得できる情報と同一である。\n  Windbg上で、DRED1.2の情報を確認する   最後に これら全てを駆使しても簡単に判明しないDEVICE_REMOVEDも存在すると思うが、DEVICE_REMOVEDを手さぐり的に解決する時代は終わりを迎えようとしていると言えると思う。\n","date":1599210000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599210000,"objectID":"a9db3b9f69d28f3162320077074647b1","permalink":"https://shikihuiku.github.io/post/cedec2020_prescriptions_for_deviceremoval/","publishdate":"2020-09-04T18:00:00+09:00","relpermalink":"/post/cedec2020_prescriptions_for_deviceremoval/","section":"post","summary":"CEDEC2020のショートセッションで説明しきれなかった部分についての補足資料になります","tags":["DX12","Debug Layer","GBV","DRED"],"title":"Device Removalの処方箋 - 補足資料","type":"post"},{"authors":[],"categories":[],"content":"HLSLのWave Intrinsicsについて Wave Intrinsicsは、HLSLのShader Model6.0から導入された新しい組み込み関数群です。 従来の他のHLSL組み込み関数が、単一スレッド内での変数のみを動作の対象するのに対して、 Wave Intrinsicsは、Waveと呼ばれる複数のスレッド間でのデータの交換や演算を行うための組み込み関数となります。 従来は、Compute Shaderなどで、他のスレッドの変数（演算用のレジスタ）が保持する値を参照するには、groupsharedで宣言された変数やUAVなどで宣言されたバッファーに情報を一旦ストアする必要があったうえ、スレッド間の同期命令が必要でした。 Wave Intrinsicsは、Wave内のスレッド間に限定されますが、他のスレッドの変数（演算用のレジスタ）の値を参照したり演算することが出来ます。 これにより、スレッド間のレジスタ空間の共有が可能になり、複数のスレッドで協調的に動作するシェーダーコードが、より記述しやすくなりました。 また、Wave内は命令実行のタイミングが同じであることが（論理上において）保証されていることから、スレッド間同期命令を必要としないのも大きな利点です。 一点注意が必要なのは、Wave IntrinsicsはShader Model 6.0以上に存在する組み込み関数ですが、実際に使用できるかどうかは、ID3D12Device::CheckFeatureSupport()で、D3D12_FEATURE_D3D12_OPTIONS1を調べる必要があります。\n用語 ここではWave Intrinsicsに関連する用語を説明します。\nWave NVIDIAの用語で\u0026quot;warp\u0026quot;とよばれ、AMDの用語では、\u0026ldquo;wavefront\u0026quot;と呼ばれてきたものです。命令発行が、同時に行われれるスレッドのグループのことです。\nLane Waveを構成する個々のスレッドを指します。\n以下の図は、一つのWaveの中に32Lane分のスレッドが存在する場合の図になります。この図式を使って様々なWave Intrinsicsについて説明していきたいと思います。   WaveとLane   Inactive Lane Waveを構成する個々のスレッドのうち、命令を実行しないスレッドを指します。\nActive Lane Waveを構成する個々のスレッドのうち、命令を実行するスレッドを指します。\n以下の図は、左側のシェーダーコードの実行に伴って変化する、Active LaneとInactive Laneの変化の例を表した図です。右側の3 Laneは、スレッド起動数等の初期条件によるInactive Laneです。 Pixel ShaderやCompute Shaderで必要とされるスレッド数が、Waveの倍数でなかった場合は、Inactive Laneの存在するWaveが起動されます。このようなInactive Laneは、状態が動的に変更されることは無く、終始Inactive Laneのままです。 3行目のIf()による分岐の条件を満たさなかったLaneは、If()ステートで囲まれたコードブロックが終了するまでInactive Laneとなります。Wave内では命令実行は暗黙的に同期する決まりになっているので、Inactive Laneはその間なにも実行せず、他のLaneが該当コードブロックの実行を完了するまで待ちます。 図にはありませんが、If()ステートのコードブロックの実行が終了すれば、条件分岐によってInactive Laneとなったスレッドは、再びActive Laneへと復帰します。\n  Active LaneとInactive Lane   Quad 先頭から連続する4Lane分づつのスレッドのグループを指します。特にPixel Shaderでは、RenderTargetにおける2x2ピクセルブロックが一つのQuadにアサインされます。 Pixel Shaderにおけるddx/ddyなどのGradient命令や、テクスチャーのLoDの計算は、Quad内の変数の差分によって実現されており、Gradientの計算のみに寄与してPixelを塗らないLane（スレッド）をHelper Laneと呼びます。\n以下の図は、とあるプリミティブをレンダリングする際の、QuadとHelper LaneのRenderTarget上での表現とWaveとしての表現の対応図です。\n  QuadとHelper Lane   Waveのサイズについて Wave Intrinsicsを使う上で、Waveのサイズというは非常に重要なファクターで、これを理解すること無しに、効率的な処理をデザインすることは難しいと思います。 NVIDIAのWarpは、伝統的に32 Lane/Waveです。対して、AMDのGCNアーキテクチャは64 Lane/Waveで動作しています。 同じくAMDのRDNAアーキテクチャは、Wave32とWave64の二つの動作モードを持ち、それぞれが、32, 64 Lane/Waveで動作しています。 どちらのモードでシェーダーが実行されるかは、ドライバーが決定するようなので、シェーダーは両モードで正しく動く必要があります。結局のところ、32 Lane/Wave、64 Lane/Waveの両方をサポートすることができれば、NVIDIA, AMDの両GPUに対応したアプリケーションとなるはずです。\n  32 Lane/Waveと64 Lane/Wave   ID3D12Device::CheckFeatureSupport()のD3D12_FEATURE_D3D12_OPTIONS1では、Wave Intrinsicsの使用の可否についてとともに、使用される可能性のあるWaveのサイズの上限値と下限値が返されます。 したがって先のRDNAの様に、単一のアーキテクチャでも、Waveのサイズは可変であると考える必要があるのかもしれません。しかし、WaveのサイズのAPI仕様としての上限値と下限値である 4 と 128 はあまりにもかけ離れているため、Waveのサイズに依存するコードを記述する際に、すべてのWaveのサイズをサポートすることは非現実的です。また、実際には使用されないWaveのサイズのためにコードを書くのも無駄だと思います。したがって、現実的な実装方法としてはD3D12_FEATURE_D3D12_OPTIONS1でWaveのサイズの上限値と下限値をチェックし、32と64の範囲ならば、Wave Intrinsicsを使ったシェーダーコードを使用し、そうでない場合はWave Intrinsicsを使用していないフォールバックのシェーダーコードを実行するか、エラーを出力して動作を終了するべきだと思います。\nWaveのサイズは、WaveGetLaneCountというWave Intrinsicsを使って取得できます。しかし、これは裏を返せば、D3D12_FEATURE_D3D12_OPTIONS1のWaveの上限値と下限値に幅がある場合は、HLSLのシェーダーコードを実行するまで、Waveのサイズが分からないという事になります。（これはAPIのデザインの問題だと思います。）\nWaveのサイズとThread Groupのサイズについて Wave Intrinsicsは、あくまでWaveのサイズを基準とした動作になっていて、Compute Shaderのnumthreadsの大きさは、Waveのサイズとは関係ありません。ただし、Wave Intrinsicsを使う場合は、numthreadsの大きさはWaveのサイズを意識したものが良いと思います。 WaveのThread Group内でのマッピングは、Row Oriented　(X軸優先）です。（ただし、これを明記しているドキュメントが見当たらなかったので注意が必要です。）numthreadsの大きさが、Waveのサイズの倍数でなかった場合は、シェーダーが実行される前からInactive Laneが存在するWaveが起動されます。この場合、Waveのサイズ分のスレッドがすべて動作していることを前提として記述されたシェーダーは、動作が破綻するので注意が必要です。 現状では、ID3D12Device::CheckFeatureSupport()のD3D12_FEATURE_D3D12_OPTIONS1の返すWaveのサイズの上限値の倍数をnumthreadsの大きさとすることで、このような事態を回避する事ができると思います。\n  numthreadとWave   PixelShaderとWave Intrinsicsについて （これも明記しているドキュメントが見当たらなかったので注意してください）\nPixel Shaderでは、すべてのWave Intrinsicsの使用が許されています。しかし、Pixel Shaderにおける描画ピクセルとWaveやLaneの対応は、描画されるプリミティブの位置と、GPUとドライバー、そしてPixel Shaderのソースコードによって決まると考えられます。 シンプルな例では、ピクセルシェーダーのスレッドは描画されるプリミティブのピクセルと一対一の関係で起動されると思います。ただし、ピクセルシェーダー内で、Gradinet命令（ddx/ddy）を使用したり、テクスチャーのサンプリングにおいて、LoDを明示的に指定しなかった場合は、スレッド間の値（テクスチャサンプリングにおいてはUV値）の差分を計算する必要があるため、起動されるスレッドは2x2ピクセル単位となります。そして、プリミティブとして描画されるピクセルを担当しているスレッドのみがRenderTargetへの出力を行います。残りのスレッドは、Helper Laneとなり、スレッドとして動作しますがRenderTargetへの出力を行いません。 プリミティブの描画においては、必要なスレッド数は必ずしもWaveのサイズの倍数とならないので、シェーダー内で条件分岐を行っていない状態でも、Inactive Laneが存在しているWaveが起動される可能性があります。また、複数のプリミティブが同一のWaveにパッキングされる可能性もあります。Pixel Shader内でWave Intrinsicsを使う場合は、これらの点について考慮する必要があると思います。\n  QuadとHelper Lane   Shader Model 6.0のWave Intrinsicsについて Shader Model 6.0のWave Intrinsicsは以下のカテゴリに分類することができます。\n Wave Query\nWaveやLaneの状態取得 Wave Vote\nWave内でのbooleanステート確認 Wave Broadcast\nWave内で特定のLaneの変数値の取得 Wave Reduction\nWave内での変数の演算 Wave Scan and Prefix\nWave内での変数の演算(自身より小さいLane Indexに限る) Quad-wide Shuffle operations\nQuadを動作対象とした、変数値の取得  Wave Query WaveのLane数と、Lane Indexを調べるためのIntrinsicsです。\n加えて、Wave内で自身が先頭のActive Laneかどうかを返す、WaveIsFirstLaneが含まれます。\nWaveGetLaneCount WaveのLaneの数を返します。全てのLaneで同じ値を受け取ります。   WaveGetLaneIndex Wave内での該当LaneのIndexを返します。個々のLaneで異なる値を受け取ります。   WaveIsFirstLane bool値を返します。ActiveLaneの中で最小のLane IndexのLaneのみtrueが返されます。残りのLaneはfalseが返されます。   Wave Vote Wave内の他のActive Laneのboolのステータスを確認するためのIntrinsicsです。\nWaveActiveAnyTrue 引数にbool値を指定します。そして、いずれかのActive Laneがtrueを渡せば、全てのActive Laneにtrueが返されます。そうでない場合は、全てのActive Laneにfalseが返されます。\n  WaveActiveAllTrue 引数にbool値を指定します。全てのActive Laneがtrueを渡せば、全てのActive Laneにtrueが返されます。そうでない場合は、全てのActive Laneにfalseが返されます。\n  WaveActiveBallot 引数にbool値を指定します。戻り値にuint4を返します。戻り値のuint4は、128bit-wideのビットマスクとなっており、各Active Laneが渡したbool値をビットマスクとして返します。Inacive Laneは暗黙的に0が設定されます。\n  Wave Broadcast Wave内で、特定のLaneの変数の値を、すべてのActive Laneで取得するためのIntrinsicsです。\nWaveReadLaneAt 引数に、読み取りの対象となる変数とLane Indexを指定します。Lane Indexで指定されたLaneの、引数で指定された変数の値を、全てのActive Laneに返します。引数で指定した変数の型と同じ型が返されます。\n他にも、引数に指定した変数の型と同じ変数型を返すタイプのWave Intrinsicsがありますが、これらはベクトル型を含め、組み込み型の整数型と浮動小数点型の殆どがサポートされています。\n  WaveReadLaneFirst 引数に、読み取りの対象となる変数を指定します。Active Laneの中で、最小のLane IndexのLaneの、引数で指定された変数の値を、すべてのActive Laneに返します。\n  Wave Reduction Wave内でのActive Laneの変数の値を用いて演算するためのIntrinsicsです。一つの演算結果がすべてのActive Laneに返されます。\nWaveActiveAllEqual 引数に、読み取りの対象となる変数を指定します。すべてのActive Laneの変数の値が等しい場合のみTrueを返します。\n  WaveActiveBitAnd 引数に、読み取りの対象となる整数型の変数を指定します。すべてのActive Laneの変数の値のBitwise AND(論理積)を演算した結果を返します。\nWaveActiveBitOr 引数に、読み取りの対象となる整数型の変数を指定します。すべてのActive Laneの変数の値のBitwise OR(論理和)を演算した結果を返します。\nWaveActiveBitXor 引数に、読み取りの対象となる整数型の変数を指定します。すべてのActive Laneの変数の値のBitwise XOR(排他的論理和)を演算した結果を返します。\n  WaveActiveCountBits 引数に、boolを指定します。引数にtrueを指定したLaneの数を、すべてのActive Laneに返します。\n  WaveActiveMax 引数に、読み取りの対象となる変数を指定します。すべてのActive Laneの変数の値の中で、最大値を、全てのActive Laneに返します。\nWaveActiveMin 引数に、読み取りの対象となる変数を指定します。すべてのActive Laneの変数の値の中で、最小値を、全てのActive Laneに返します。\nWaveActiveProduct 引数に、読み取りの対象となる変数を指定します。すべてのActive Laneの変数をの値を乗算した結果を、全てのActive Laneに返します。 演算の順序については、API仕様としての明確な定義が無いので、扱う変数の型や、値の範囲について注意が必要です。\nWaveActiveSum 引数に、読み取りの対象となる変数を指定します。すべてのActive Laneの変数の値を加算した結果を、全てのActive Laneに返します。 演算の順序については、API仕様としての明確な定義が無いので、扱う変数の型や、値の範囲について注意が必要です。\n  Wave Scan and Prefix Wave Reduction系に似ていますが、演算の対象が自身のLane Index未満のActive Laneのみです。自身のLaneは演算の対象に含みません。 演算の結果は、基本的にはLaneごとに異なる値が返されることになります。\nWavePrefixCountBits 引数にboolを指定します。自身のLane Index未満のActive Laneで、引数にtrueを指定した個数を返します。\nWavePrefixSum 引数に、読み取りの対象となる変数を指定します。自身のLane Index未満のActive Laneの、変数の値を加算した結果を返します。 演算の順序については、API仕様としての明確な定義が無いので、扱う変数の型や、値の範囲について注意が必要です。[precise]フラグは無視されます。\nWavePrefixProduct 引数に、読み取りの対象となる変数を指定します。自身のLane Index未満のActive Laneの、変数の値を乗算した結果を返します。 演算の順序については、API仕様としての明確な定義が無いので、扱う変数の型や、値の範囲について注意が必要です。[precise]フラグは無視されます。\n  Quad-wide Shuffle operations Pixel Shaderでのみ使用可能なWave Intrinsicsです。 (これについては、2020/08現在ドキュメントの表記と実装に食い違いがあります。ドキュメントにはCompute Shaderでも使用可能と表記されており、その場合、Lane Indexの0より4 Laneごとに区切ったLaneがQuadとして扱われるとされています。 しかし実際には、Quad系を使用したCompute Shaderのコンパイル時にopcode 'QuadReadAcross' should only be used in 'Pixel Shader'というメッセージが出力されます。そして、シェーダーの生成にも失敗します。)\nQuadReadLaneAt 引数に、Quad内のローカルのLane Indexと、読み取り対象となる変数を指定します。Quad内で同じ値が返されます。 読み取り対象LaneがInactive Laneだった場合の読み取り結果は未定義なので注意が必要です。 Pixel ShaderにおけるQuad内のローカルのLane Indexは、下図に示した通りRow Orientedとなっています。\nQuadReadAcrossDiagonal 引数に、読み取り対象となる変数を指定します。Quad内で互いに対角の位置にあるLaneの値を読み取ります。(例えば、Lane:0はLane:3の値を受け取ります。) (APIドキュメントに明記がありませんが、読み取り対象LaneがInactive Laneだった場合の読み取り結果は未定義なので注意が必要です。)\nQuadReadAcrossX 引数に、読み取り対象となる変数を指定します。Quad内で互いに水平の位置にあるLaneの値を読み取ります。(例えば、Lane:0はLane:1の値を受け取ります。) (APIドキュメントに明記がありませんが、読み取り対象LaneがInactive Laneだった場合の読み取り結果は未定義なので注意が必要です。)\nQuadReadAcrossY 引数に、読み取り対象となる変数を指定します。Quad内で互いに垂直の位置にあるLaneの値を読み取ります。(例えば、Lane:0はLane:3の値を受け取ります。) (APIドキュメントに明記がありませんが、読み取り対象LaneがInactive Laneだった場合の読み取り結果は未定義なので注意が必要です。)\n  Shader Model 6.5のWave Intrinsicsについて Model 6.5で、いくつかの新しいWaveIntrinsicsが導入されています。\nWaveMatch 引数に、読み取り対象となる変数を指定します。\n戻り値にuint4を返します。戻り値のuint4は、128bit-wideのビットマスクとなっており、各Active Laneの引数で指定された変数の値が、自身のLaneの変数の値と等しい場合に、ビットがセットされます。Inacive Laneは暗黙的に0が設定されます。\n  WaveMultiPrefixSum 引数に、読み取り対象となる変数を指定します。また、引数に128bit-wideのビットマスクとなる uint4 を指定します。\nWaveActiveSumと動作は似ていますが、加算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。 ビットマスクは、Laneごとに設定を変更出来ますが、一つのLaneは1種類のビットマスクにしか所属する事ができません。 つまり、ビットマスクによって、Laneをパーティショニングしてサブセット化する事が出来ますが、各々のLaneが完全に自由にビットマスクを指定できるわけではありません。一つのLaneが複数の種類のビットマスクに所属した場合の動作は未定義です。\nWaveのサイズを超えるBitやInactive Laneのビットは無視されます。(ビットがゼロとして扱います。) このビットマスクの仕様は他のWaveMultiPrefix系と共通です。\n  WaveMultiPrefixProduct 引数に、読み取り対象となる変数を指定します。また、引数に128bit-wideのビットマスクとなるuint4を指定します。\nWaveActiveProductと動作は似ていますが、乗算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。\nWaveMultiPrefixCountBit 引数に、bool値を指定します。また、引数に128bit-wideのビットマスクとなるuint4を指定します。\nWaveActiveCountBitと動作は似ていますが、乗算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。\nWaveMultiPrefixAnd 引数に、読み取り対象となる整数型の変数を指定します。また、引数に128bit-wideのビットマスクとなるuint4を指定します。\nWaveActiveBitAndと動作は似ていますが、乗算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。\nWaveMultiPrefixOr 引数に、読み取り対象となる整数型の変数を指定します。また、引数に128bit-wideのビットマスクとなるuint4を指定します。\nWaveActiveBitOrと動作は似ていますが、乗算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。\nWaveMultiPrefixXor 引数に、読み取り対象となる整数型の変数を指定します。また、引数に128bit-wideのビットマスクとなるuint4を指定します。\nWaveActiveBitXorと動作は似ていますが、乗算の対象となるLaneがビットマスクで指定されたLaneに限定される点が異なります。\n終わりに 今回は、Wawve Intrinsicsの動作を理解するための基本的な内容となっているので、実際の使用ケースについては言及しませんでした。 次回は、もう少し実際の利用ケースについて触れたいと思います。\n","date":1597575632,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597575632,"objectID":"d26efcbc36e3445c1245aed40fbb6ffc","permalink":"https://shikihuiku.github.io/post/wave_intrinsics1/","publishdate":"2020-08-16T20:00:32+09:00","relpermalink":"/post/wave_intrinsics1/","section":"post","summary":"Wave Intrinsicsの動作を理解しよう","tags":["D3D12","WaveIntrinsics"],"title":"HLSLのWave Intrinsicsについて","type":"post"},{"authors":[],"categories":[],"content":"動機とか タイトルの画像は、今まで運用してきたWordpress上のサイトのスクリーンショットです。記念に撮ってきました。\n別にWordpressがいやになったという訳ではないのですが、Github pagesに移行したほうが制約も少なく扱いやすい気がしたので引っ越しすることにしました。Wordpressに書いた記事は、簡単に移行するのは難しそうなので、そのままにしておきます。\nHugo＋Academic 別に十分な検討をしてこの組み合わせに至ったわけでは無く、静的サイト生成ツール＋なんか都合の良いTheme程度の認識で選択しました。今後変えるかもしれません。 ただ、コンテンツは多少特殊な要素があったとしても、基本的にMarkdownで記述できるので、今後もしサイトを移行しようと思っても、記事の移行をあきらめたくなるような事はないのではないでしょうか。\n導入手順 せかっくなので自分なりの導入手順を記しておきます。環境はWindows10を使用しています。Hugoは導入済です。\nAcademicの導入 まず、Hugoのテーマとして、Academicを導入しようとして、以下の様にファイルを配置しましたが、上手くいきませんでした。\ngit submodule add https://github.com/gcushen/hugo-academic.git themes/academic   Academicのドキュメントを参照すると、Hugoの新規サイトの状態に加えて、いろいろなファイルが正しい位置に配置されている必要があるようで、 academic-kickstart.gitをクローンすることがおすすめのようです。 初めはプライベートリポジトリとして扱いたいですし、リポジトリの名前も変えたいので、cloneしてmirrorします。\ngit clone --bare https://github.com/sourcethemes/academic-kickstart.git cd academic-kickstart.git git push --mirror https://github.com/shikihuiku/blog.git  早速ローカルで初期状態を確認しようと思ったら、ビルドエラーが出ました。\n\u0026gt; hugo server Building sites … ERROR 2020/07/25 17:30:35 render of \u0026quot;term\u0026quot; failed: execute of template failed: template: authors/list.html:5:3: executing \u0026quot;authors/list.html\u0026quot; at \u0026lt;partial \u0026quot;site_head\u0026quot; .\u0026gt;: error calling partial: \u0026quot;T:\\GitHub\\hugotest\\blog\\themes\\academic\\layouts\\partials\\site_head.html:131:56\u0026quot;: execute of template failed: template: partials/site_head.html:131:56: executing \u0026quot;partials/site_head.html\u0026quot; at \u0026lt;resources.Concat\u0026gt;: error calling Concat: resources in Concat must be of the same Media Type, got \u0026quot;text/x-scss\u0026quot; and \u0026quot;text/css\u0026quot; ERROR 2020/07/25 17:30:35 render of \u0026quot;section\u0026quot; failed: execute of template failed: template: section/publication.html:5:3: executing \u0026quot;section/publication.html\u0026quot; at \u0026lt;partial \u0026quot;site_head\u0026quot; .\u0026gt;: error calling partial: \u0026quot;T:\\GitHub\\hugotest\\blog\\themes\\academic\\layouts\\partials\\site_head.html:131:56\u0026quot;: execute of template failed: template: partials/site_head.html:131:56: executing \u0026quot;partials/site_head.html\u0026quot; at \u0026lt;resources.Concat\u0026gt;: error calling Concat: resources in Concat must be of the same Media Type, got \u0026quot;text/x-scss\u0026quot; and \u0026quot;text/css\u0026quot; ERROR 2020/07/25 17:30:35 render of \u0026quot;home\u0026quot; failed: execute of template failed: template: index.html:5:3: executing \u0026quot;index.html\u0026quot; at \u0026lt;partial \u0026quot;site_head\u0026quot; .\u0026gt;: error calling partial: \u0026quot;T:\\GitHub\\hugotest\\blog\\themes\\academic\\layouts\\partials\\site_head.html:131:56\u0026quot;: execute of template failed: template: partials/site_head.html:131:56: executing \u0026quot;partials/site_head.html\u0026quot; at \u0026lt;resources.Concat\u0026gt;: error calling Concat: resources in Concat must be of the same Media Type, got \u0026quot;text/x-scss\u0026quot; and \u0026quot;text/css\u0026quot; ERROR 2020/07/25 17:30:35 render of \u0026quot;taxonomy\u0026quot; failed: execute of template failed: template: authors/terms.html:5:3: executing \u0026quot;authors/terms.html\u0026quot; at \u0026lt;partial \u0026quot;site_head\u0026quot; .\u0026gt;: error calling partial: \u0026quot;T:\\GitHub\\hugotest\\blog\\themes\\academic\\layouts\\partials\\site_head.html:131:56\u0026quot;: execute of template failed: template: partials/site_head.html:131:56: executing \u0026quot;partials/site_head.html\u0026quot; at \u0026lt;resources.Concat\u0026gt;: error calling Concat: resources in Concat must be of the same Media Type, got \u0026quot;text/x-scss\u0026quot; and \u0026quot;text/css\u0026quot; ERROR 2020/07/25 17:30:35 failed to render pages: render of \u0026quot;section\u0026quot; failed: execute of template failed: template: section/talk.html:5:3: executing \u0026quot;section/talk.html\u0026quot; at \u0026lt;partial \u0026quot;site_head\u0026quot; .\u0026gt;: error calling partial: \u0026quot;T:\\GitHub\\hugotest\\blog\\themes\\academic\\layouts\\partials\\site_head.html:131:56\u0026quot;: execute of template failed: template: partials/site_head.html:131:56: executing \u0026quot;partials/site_head.html\u0026quot; at \u0026lt;resources.Concat\u0026gt;: error calling Concat: resources in Concat must be of the same Media Type, got \u0026quot;text/x-scss\u0026quot; and \u0026quot;text/css\u0026quot; Built in 84 ms Error: Error building site: TOCSS: failed to transform \u0026quot;main_parsed.scss\u0026quot; (text/x-scss): resource \u0026quot;scss/scss/main.scss_76ac6956597c32fec7ddf60d408db3ab\u0026quot; not found in file cache  調べてみると、Academicには、hugo_extendedが必要だという事が分かりましたので、 Hugo_extendedのビルド済バイナリをDLします。 再びローカルサーバーを立ち上げると、今回は上手くビルドできました。\nhugo server  ローカルホストのポート1313にアクセスすると、おしゃれなサイトが表示されました。BiographyやProjectsやPublicationsなど、かなりハイスペック人材向けのテンプレートで尻込みしますが、どんどん削っていくことにします。\nAcademicのカスタマイズ ここで先人の知恵をお借りします\n Hugo + Academic テーマを使ったブログの作り方 https://qiita.com/harumaxy/items/58e7e4273c61e7e260b3\n /config/_default/フォルダに格納されている以下のtomlファイルを編集していきます。\n config.toml language.toml menus.toml params.toml  その他諸々の変更を行って、シンプルにBlogのポストができるページにしました。言語設定はenのまま使用する事にします。\nフォントの設定 デフォルトでは、GoogleのWebフォントがいろいろ指定されていますが、フォントの設定はシンプルな方が良いと思っています。 フォントのプリセットにNativeという設定があり、こちらを使うとrootのfont-familyの設定を、殆どの要素で使うようになるようです。config/_default/params.tomlでこれを指定します。\n font=\u0026ldquo;Native\u0026rdquo;\n rootにある、フォントの設定はとりあえず変更せずに使ってみます。\ncustom.scssの設定 デフォルトではブラウザの横幅に対してページの表示領域が酷く狭いです。\nblog/assets/scss/custom.scssというファイルを配置することで、自身で記述したcssをページに読み込ませる事が出来るようです。生成されたHTMLの要素のクラス名を確認して適当に設定しました。なんだか横幅を変えるだけで泥臭い作業になりました。もっと簡単にスタイルを変更する方法があるかもしれません。\n/*width for top page*/ .container { max-width: 90%; } /*width for posts.*/ .article-container{ max-width:90% }  また、見出しのフォントのWeightが一定では無いので変更します。ついでにマージンも変更します。 この辺りは素人なので、あまり参考になりませんが。 しかし、CSSを書いて変更できると結局楽だなってなって思ってきました\u0026hellip;\nh1, h2, h3, h4, h5, h6 { margin-top: 1.7rem; margin-bottom: 0.3rem; font-weight: 700; }  アイコンの設定 それから、Webサイトのアイコン、所謂ファビコンがデフォルトの設定では、Academicのアイコンになっているので変更します。\nassets/images/フォルダに、解像度512x512のicon.pngを配置します。\nGithub Pagesの設定 最後に、実際にビルドされたページをGithub Pagesでホスティングする方法ですが、一番簡単な方法はHugoの出力先をdocsフォルダにして、それをそのままリポジトリにPushして、GithubPagesで公開する方法だと思います。 Hugoはデフォルトではpublicフォルダにファイルが生成されるので、これを変更します。\nconfig/_default/config.tomlに以下の様に設定することで、docsフォルダにサイトが生成されるようになります。\npublishdir= \u0026quot;docs\u0026quot;  あとは、baseurlを設定しサイトを生成してエラーがでなければOKです。リポジトリにPushして、github pagesの設定をすれば公開されます。\nGithub Pagesの設定 - やっぱりPrivateリポジトリで 構築履歴が閲覧可能な状態なのは別に構わないですし、大抵の場合はDraft記事が閲覧可能な状態でも構わないのですが、一部のCEDECセッションの補間資料とかのDraftは会期以前に公開状態になるのはまずいので、 publishしたべージのコンテンツのみを公開状態にする必要があります。結局Hugoのpublishdirのディレクトリ以下を別のリポジトリにして、こちらだけPublicに設定して、ビルド環境はPrivateリポジトリにすることにしました。\n","date":1595462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595462400,"objectID":"1a9a117957ea8d71e8aeec5a04f15ce9","permalink":"https://shikihuiku.github.io/post/hello_hugo_and_academic/","publishdate":"2020-07-23T00:00:00Z","relpermalink":"/post/hello_hugo_and_academic/","section":"post","summary":"永らくWordpressでダラダラと記事をPostしてきましたが、Wordpressであることのメリットがほとんど活かせてない状況です。自分は静的サイトが作れれば十分なので、引っ越すことにしました。","tags":["Hugo","Academic"],"title":"Hugo+Academicでブログを構築","type":"post"},{"authors":[],"categories":[],"content":"この記事は、旧サイトからテスト用に移植しました。  入力遅延の問題はゲーム開発において悩ましい問題の一つです。特にPCでは、他のプロセスが勝手な都合で動作しますし、リソースの競合も発生します。また、PC本体のCPU/GPUパフォーマンスの違いも大きいです。ここでは一般的なデスクトップPC上で、フォートナイトの描画がどのように実行されているかをソフトウェアの見地から、GPUViewを用いて観測してみます。ここで言う入力遅延は、Windows上のゲームのプロセスがキーやマウスのステートを取得すると思われるタイミングから、描画された画面が、ディスプレイへの出力対象になるまでを指します。実際には、マウスやキーボードのハードウェアとドライバにも遅延がありますし、ディスプレイにも実際に輝点として可視化されるまでに遅延がありますが、これらは今回は考慮しません。またゲームのプロセスが正確にいつ入力デバイスの情報を取得しているかは考慮しません。あるフレームのCPU処理の開始を入力取得時間として考えます。\nちなみに今回使用したデスクトップPCは、Core-i7 7700KとGeForce RTX2080Tiが搭載されています。モニタは一般的な60Hzの4Kディスプレイです。\nテストに使ったシーンは、クリエイティブの島です。描画としては極めて軽い状態がテスト対象です。\nUE4の開発者の方は、すでにご存じかと思いますが、以下の資料に入力遅延に関する詳しい解説がご覧いただけると思います。\n\n UE4のスレッドの流れと Input Latency改善の仕組み  from エピック・ゲームズ・ジャパン Epic Games Japan 今回は、フォートナイトをプレイする上でどのような設定が一番自分にとって好ましいかを調べる過程で分かったことを説明していきます。フォートナイトの描画設定で、入力遅延に関係のある設定は、フレームレートの上限値、VSync、それから、マルチスレッドレンダリングです。描画APIはD3D11と12に対応していますが、D3D12にすることによる利点があまり感じられなかったため、今回はD3D11のみをテストしています。\nVSync: OFF マルチスレッドレンダリング: OFF フレームレート上限: 60   vsyncoff_mtoff_60   おなじみのGPUViewのログです。詳しく見たい方はクリックして拡大してください。まずは、スレッドのアクティビティを理解するために一番簡単な例を示します。VSyncがOffなので、青い縦線で示されたVSyncのタイミングとは全く関係なく描画されています。描画スレッドが、D3D11の描画APIを呼び出して、Present()を呼び出し、GPUが描画を完了するのとほぼ同時にフロントバッファへのFlipが行われ、ディスプレイへの表示対象になります。ドライバのスレッドに描画命令を発行しているスレッドが、UE4のRHIスレッドと思われますが、マルチスレッドレンダリングをOffにしているので、Renderのスレッドが、直接RHIを呼び出しているのではないかと思われます。それに先立ち動作しているスレッドがゲームのメインスレッドと思われます。ゲームのメインスレッドは、Render/RHIスレッドに渡す描画情報を構築するタイミングと思われるところで、フレームレートのペーシングを行っていると思われます。計測された入力遅延は、12.8msですが、CPUもGPUもアイドル時間が長いので、処理クロックを落としていると思われます。実際の場合も何も小細工しなければ、ユーザーの知らないところでクロックが下がるので、今回はこの設定の入力遅延は12ms前後と考えます。\nVSync: OFF マルチスレッドレンダリング: ON フレームレート上限: 60 次は、先ほどの設定から、マルチスレッドレンダリングを有効にしてみます。他の設定は同じです。\n  vsyncoff_mton_60   見た目ががらりと変わっていますが、アイドリングしているWorkerスレッドにRenderと思われるスレッドが埋もれてしまったため、このような見た目になっています。実際は、どの設定でも多数のWorkerスレッドやサウンドのスレッドが立ち上げられていますが、描画に関係ないものは省略しています。先ほどの例と異なり、RHIのスレッドと思われるスレッドと、Renderと思われるスレッドが別になりました。基本的な仕組みや、遅延の状況はほぼ同じです。こちらもおそらく動作クロックが下がっているので、本来の描画パフォーマンスと比べると処理時間がかかっています。\nVSync: OFF マルチスレッドレンダリング: OFF フレームレート上限: 120   vsyncoff_mtoff_120   次は、再びマルチスレッドレンダリングをOFFに戻しました。そして、フレームレート上限を120にしてみました。基本的な動作は一番初めの例と同じですが、フレームのペーシングが8.3msになったことで、120FPSのレンダリングになりました。そして、アイドリングのデューティ比が変わったことにより、動作クロックが引き上げられた関係で、入力遅延も短縮され、9.5ms程度になりました。\nVSync: OFF マルチスレッドレンダリング: ON フレームレート上限: 120 次は、上記の設定でマルチスレッドレンダリングをONにします。   vsyncoff_mton_120   やはり、RenderスレッドとRHIスレッドが分かれました。今回のテスト対象になっているシーンは軽いので、マルチスレッドレンダリングの恩恵は少ないですが、もっと複雑なシーンでは、これらのスレッドが並列動作することにより、Renderスレッドの描画命令発行によるストールが軽減され、より顕著な差になると思われます。少なくとも遅くなることはなさそうなので、私はこの設定でプレイすることにします。\nVSync: ON マルチスレッドレンダリング: OFF フレームレート上限: 制限なし 次は、いわゆる、VSyncを守って、画面のティアリングを起こさない描画になります。見た目は一番スムーズなのですが、入力遅延の観点からはあまりお勧めできない設定となりそうです。   vsyncon_mtoff_60   まず、VSyncを取ると、メインスレッドの動作がかなり変わります。およそ2ms単位でスレッドをポーリングしながら、処理開始のタイミングを計っているようです。rhi.SyncSlackMSのデフォルト設定と思われる、VSyncの10ms前に、メインスレッドの処理を開始しています。Renderスレッドは、前の前のフレームのGPU描画処理が完了してから、RHIの呼び出しを開始しているようです。そして、RHIが呼び出すD3DAPIによって生成されたGPUタスクは、ドライバのGPUタスクキューに積み上げられます。そのフレームのGPU描画処理がGPU上で実行されるのは、Renderスレッドが動作したフレームの次の次のフレームです。そして、ディスプレイの出力対象になるFlipが行われるのは、VSyncに同期しているので、実際の表示はその次のフレームとなります。メインスレッドが動作を開始してから、ディスプレイの出力対象になるまでの入力遅延は60msほどとなります。\nまとめ 少なくとも私の環境では、VSyncをOFFにして、CPUのフレームペーシングがボトルネックになる状態（つまりGPUの処理時間には余裕がある状態）で、なるべく高いフレームレートが入力遅延が一番小さくなる状態だといえると思います。入力遅延を最短にするという目的ならば、私のPCでは、おそらく200フレーム以上の設定の方が短くなると思われます。しかし、使用しているディスプレイも60Hzですし、描画解像度など他の設定に妥協が必要になります。 また、これらの状況は、個々のPCの、CPUとGPUの処理能力のバランスによって変動するので、皆さんに一概にこの設定がおすすめですとはなりません。しかし、VSyncはOff、なるべく早いCPU、なるべく早いGPU、なるべく軽い描画が、入力遅延低減につながると思われます。\n","date":1592238782,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592238782,"objectID":"117dad02d1f9ebb737d4f2c72538fd7f","permalink":"https://shikihuiku.github.io/post/check_input_latency_of_fortnite/","publishdate":"2020-06-16T01:33:02+09:00","relpermalink":"/post/check_input_latency_of_fortnite/","section":"post","summary":"タイトルは緩いですけど、ゲーム開発者向けの話です。プラットフォームはPC限定です。","tags":["DX11","GPUView","UE4","Fortnite"],"title":"フォートナイトの入力遅延を観測してみた","type":"post"}]